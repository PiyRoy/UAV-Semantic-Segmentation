{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9917705,"sourceType":"datasetVersion","datasetId":6094874}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom glob import glob\nimport numpy as np\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Add, GlobalAveragePooling2D, Dense, MaxPooling2D, Input, UpSampling2D\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport time\nimport matplotlib.colors as mcolors\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Model","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-15T16:59:08.888198Z","iopub.execute_input":"2024-11-15T16:59:08.888986Z","iopub.status.idle":"2024-11-15T16:59:08.896020Z","shell.execute_reply.started":"2024-11-15T16:59:08.888942Z","shell.execute_reply":"2024-11-15T16:59:08.895066Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 6\nlr = 1e-3\nepochs = 100\nwidth = 512\nheight = 512","metadata":{"execution":{"iopub.status.busy":"2024-11-15T16:59:10.863539Z","iopub.execute_input":"2024-11-15T16:59:10.864294Z","iopub.status.idle":"2024-11-15T16:59:10.868669Z","shell.execute_reply.started":"2024-11-15T16:59:10.864256Z","shell.execute_reply":"2024-11-15T16:59:10.867634Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_path = os.path.join(\"/kaggle/input/aeroscapess/aeroscapes\")\nfiles_dir = os.path.join(\"files\", \"modified_uavid_dataset\")\nmodel_file = os.path.join(files_dir, \"UnetModel.keras\")\nlog_file = os.path.join(files_dir, \"Log-Unet.csv\")\n\n# Function to create directory\ndef create_dir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n        \ncreate_dir(files_dir)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T16:59:12.533065Z","iopub.execute_input":"2024-11-15T16:59:12.533423Z","iopub.status.idle":"2024-11-15T16:59:12.539969Z","shell.execute_reply.started":"2024-11-15T16:59:12.533389Z","shell.execute_reply":"2024-11-15T16:59:12.538964Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_data(path):\n    Images = sorted(glob(os.path.join(path, \"JPEGImages\", \"*\")))\n    Labels = sorted(glob(os.path.join(path, \"Visualizations\", \"*\")))\n\n    return (Images, Labels)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T16:59:14.133406Z","iopub.execute_input":"2024-11-15T16:59:14.134279Z","iopub.status.idle":"2024-11-15T16:59:14.140909Z","shell.execute_reply.started":"2024-11-15T16:59:14.134226Z","shell.execute_reply":"2024-11-15T16:59:14.139846Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"(Images, Labels) = load_data(dataset_path)\n\nprint(f\"New Train: {len(Images)} - {len(Labels)}\")\n\n# First, split off 10% of the data for testing\ntrain_val_images, test_x, train_val_labels, test_y = train_test_split(Images, Labels, test_size=0.1, random_state=42)\n\n# Then, split the remaining 90% into 70% training and 20% validation (0.7 / 0.9 â‰ˆ 0.78)\ntrain_x, valid_x, train_y, valid_y = train_test_split(train_val_images, train_val_labels, test_size=0.22, random_state=42)\n\nprint(f\"Training set: {len(train_x)} images\")\nprint(f\"Validation set: {len(valid_x)} images\")\nprint(f\"Test set: {len(test_x)} images\")","metadata":{"execution":{"iopub.status.busy":"2024-11-15T16:59:15.367882Z","iopub.execute_input":"2024-11-15T16:59:15.368284Z","iopub.status.idle":"2024-11-15T16:59:15.654104Z","shell.execute_reply.started":"2024-11-15T16:59:15.368244Z","shell.execute_reply":"2024-11-15T16:59:15.653084Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def read_image(path):\n    path = path.decode()\n    x = cv2.imread(path, cv2.IMREAD_COLOR)\n    x = cv2.resize(x, (width, height))\n    x = x / 255.0\n    x = x.astype(np.float32)\n    return x","metadata":{"execution":{"iopub.status.busy":"2024-11-15T16:59:17.268737Z","iopub.execute_input":"2024-11-15T16:59:17.269581Z","iopub.status.idle":"2024-11-15T16:59:17.274881Z","shell.execute_reply.started":"2024-11-15T16:59:17.269541Z","shell.execute_reply":"2024-11-15T16:59:17.273920Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"color_map = {\n    (0, 0, 0): 0,            # Background\n    (192, 128, 128): 1,      # Person\n    (0, 128, 0): 2,          # Bike\n    (128, 128, 128): 3,      # Car\n    (128, 0, 0): 4,          # Drone\n    (0, 0, 128): 5,          # Boat\n    (192, 0, 128): 6,        # Animal\n    (192, 0, 0): 7,          # Obstacle\n    (192, 128, 0): 8,        # Construction\n    (0, 64, 0): 9,           # Vegetation\n    (128, 128, 0): 10,       # Road\n    (0, 128, 128): 11,       # Sky\n}\n\ndef read_mask(path):\n    path = path.decode()\n    mask = cv2.imread(path)  \n    mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB) \n    mask = cv2.resize(mask, (width, height), interpolation=cv2.INTER_NEAREST)\n\n    class_indices = np.zeros((height, width), dtype=np.uint8)\n\n    for rgb, idx in color_map.items():\n        class_indices[(mask == rgb).all(axis=-1)] = idx\n\n    return class_indices.astype(np.uint8)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T16:59:19.118303Z","iopub.execute_input":"2024-11-15T16:59:19.118711Z","iopub.status.idle":"2024-11-15T16:59:19.128316Z","shell.execute_reply.started":"2024-11-15T16:59:19.118673Z","shell.execute_reply":"2024-11-15T16:59:19.127321Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tf_parse(x, y):\n    def _parse(x, y):\n        x = read_image(x) \n        y = read_mask(y) \n        return x, y\n    \n    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.uint8]) \n    x.set_shape([height, width, 3]) \n    y.set_shape([height, width])   \n    return x, y","metadata":{"execution":{"iopub.status.busy":"2024-11-15T16:59:22.348485Z","iopub.execute_input":"2024-11-15T16:59:22.348914Z","iopub.status.idle":"2024-11-15T16:59:22.355275Z","shell.execute_reply.started":"2024-11-15T16:59:22.348877Z","shell.execute_reply":"2024-11-15T16:59:22.354319Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tf_dataset(x, y, batch=6):\n    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n    dataset = dataset.map(tf_parse, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.batch(batch)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2024-11-15T16:59:23.668638Z","iopub.execute_input":"2024-11-15T16:59:23.669058Z","iopub.status.idle":"2024-11-15T16:59:23.677528Z","shell.execute_reply.started":"2024-11-15T16:59:23.669020Z","shell.execute_reply":"2024-11-15T16:59:23.676643Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = tf_dataset(train_x, train_y, batch=batch_size)\nvalid_dataset = tf_dataset(valid_x, valid_y, batch=batch_size)\ntest_dataset = tf_dataset(test_x, test_y, batch=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T16:59:25.407944Z","iopub.execute_input":"2024-11-15T16:59:25.408341Z","iopub.status.idle":"2024-11-15T16:59:26.243179Z","shell.execute_reply.started":"2024-11-15T16:59:25.408304Z","shell.execute_reply":"2024-11-15T16:59:26.241974Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a custom color map based on the color_map dictionary\ncolor_map_values = list(color_map.keys())\ncolor_map_rgb = np.array(color_map_values) / 255.0  # Normalize to 0-1 range for matplotlib\n\ndef plot_samples_with_labels(dataset, title):\n    plt.figure(figsize=(12, 12))\n    for i, (images, masks) in enumerate(dataset.take(1)):  # Take a single batch\n        for j in range(4):  # Loop over first four images\n            # Plot the image\n            plt.subplot(4, 4, j*2 + 1)\n            plt.imshow(images[j])\n            plt.axis(\"off\")\n            plt.title(f\"{['Train', 'Validation', 'Test'][title]} Image {j+1}\")\n\n            # Convert class indices in the mask to RGB colors\n            mask_rgb = np.zeros((height, width, 3), dtype=np.float32)\n            for idx, color in enumerate(color_map_rgb):\n                mask_rgb[masks[j] == idx] = color\n\n            # Plot the label mask\n            plt.subplot(4, 4, j*2 + 2)\n            plt.imshow(mask_rgb)\n            plt.axis(\"off\")\n            plt.title(f\"{['Train', 'Validation', 'Test'][title]} Mask {j+1}\")\n    \n    plt.tight_layout()\n    plt.show()\n\n# Plot the images and corresponding label masks for each dataset\nplot_samples_with_labels(train_dataset, title=0)   # Train images and masks\nplot_samples_with_labels(valid_dataset, title=1)   # Validation images and masks\nplot_samples_with_labels(test_dataset, title=2)    # Test images and masks","metadata":{"execution":{"iopub.status.busy":"2024-11-15T16:59:27.488231Z","iopub.execute_input":"2024-11-15T16:59:27.489134Z","iopub.status.idle":"2024-11-15T16:59:33.463894Z","shell.execute_reply.started":"2024-11-15T16:59:27.489092Z","shell.execute_reply":"2024-11-15T16:59:33.462933Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def encoder_block(filters, kernel_size=(3, 3), activation='relu'):\n  return tf.keras.Sequential([\n      layers.Conv2D(filters, kernel_size, padding='same'),\n      layers.BatchNormalization(),\n      layers.Activation(activation),\n      layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n  ])","metadata":{"execution":{"iopub.status.busy":"2024-11-15T16:59:33.465893Z","iopub.execute_input":"2024-11-15T16:59:33.466428Z","iopub.status.idle":"2024-11-15T16:59:33.473185Z","shell.execute_reply.started":"2024-11-15T16:59:33.466354Z","shell.execute_reply":"2024-11-15T16:59:33.472181Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def decoder_block(filters, kernel_size=(3, 3), activation='relu'):\n  return tf.keras.Sequential([\n      layers.UpSampling2D((2, 2)),\n      layers.Conv2D(filters, kernel_size, padding='same'),\n      layers.BatchNormalization(),\n      layers.Activation(activation)\n  ])","metadata":{"execution":{"iopub.status.busy":"2024-11-15T16:59:33.474552Z","iopub.execute_input":"2024-11-15T16:59:33.474941Z","iopub.status.idle":"2024-11-15T16:59:33.486369Z","shell.execute_reply.started":"2024-11-15T16:59:33.474895Z","shell.execute_reply":"2024-11-15T16:59:33.485289Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@tf.keras.utils.register_keras_serializable(package='Custom', name='BAM')\nclass BAM(tf.keras.layers.Layer):\n    def __init__(self, channel, reduction=16, dilation_conv_num=2, **kwargs):  # Accept arbitrary kwargs\n        super(BAM, self).__init__(**kwargs)\n        \n        # Channel Attention Module\n        self.channel_avg_pool = layers.GlobalAveragePooling2D()\n        self.channel_fc1 = layers.Dense(channel // reduction, activation='relu')\n        self.channel_fc2 = layers.Dense(channel)\n        \n        # Spatial Attention Module\n        self.spatial_conv1 = layers.Conv2D(channel // reduction, kernel_size=1, activation='relu')\n        self.spatial_dilated_convs = [\n            layers.Conv2D(channel // reduction, kernel_size=3, padding='same', dilation_rate=dilation, activation='relu')\n            for dilation in range(1, dilation_conv_num + 1)\n        ]\n        self.spatial_conv2 = layers.Conv2D(1, kernel_size=1)\n\n    def call(self, x):\n        # Channel Attention\n        channel_attention = self.channel_avg_pool(x)\n        channel_attention = self.channel_fc1(channel_attention)\n        channel_attention = self.channel_fc2(channel_attention)\n        channel_attention = tf.nn.sigmoid(channel_attention)\n        channel_attention = tf.reshape(channel_attention, [-1, 1, 1, x.shape[-1]])\n        channel_refined = x * channel_attention\n\n        # Spatial Attention\n        spatial_attention = self.spatial_conv1(channel_refined)\n        for conv in self.spatial_dilated_convs:\n            spatial_attention = conv(spatial_attention)\n        spatial_attention = self.spatial_conv2(spatial_attention)\n        spatial_attention = tf.nn.sigmoid(spatial_attention)\n        \n        # Combining Attention\n        refined_feature = x * spatial_attention + x * channel_attention\n        return refined_feature","metadata":{"execution":{"iopub.status.busy":"2024-11-15T16:59:37.444116Z","iopub.execute_input":"2024-11-15T16:59:37.444848Z","iopub.status.idle":"2024-11-15T16:59:37.456686Z","shell.execute_reply.started":"2024-11-15T16:59:37.444806Z","shell.execute_reply":"2024-11-15T16:59:37.455638Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def SegNet(input_shape):\n    inputs = layers.Input(shape=input_shape)\n\n    # Encoder with BAM after each encoder block\n    e1 = encoder_block(64)(inputs)\n    e1 = BAM(64)(e1)  # BAM applied to encoder block 1 output\n    e2 = encoder_block(128)(e1)\n    e2 = BAM(128)(e2)  # BAM applied to encoder block 2 output\n    e3 = encoder_block(256)(e2)\n    e3 = BAM(256)(e3)  # BAM applied to encoder block 3 output\n    e4 = encoder_block(512)(e3)\n    e4 = BAM(512)(e4)  # BAM applied to encoder block 4 output\n    \n    # Decoder with BAM before concatenation\n    d1 = decoder_block(512)(e4)\n    d1 = BAM(512)(d1)  # BAM before concatenating with e3\n    d1 = layers.concatenate([d1, e3], axis=-1)\n    d2 = decoder_block(256)(d1)\n    d2 = BAM(256)(d2)  # BAM before concatenating with e2\n    d2 = layers.concatenate([d2, e2], axis=-1)\n    d3 = decoder_block(128)(d2)\n    d3 = BAM(128)(d3)  # BAM before concatenating with e1\n    d3 = layers.concatenate([d3, e1], axis=-1)\n    d4 = decoder_block(64)(d3)\n\n    outputs = layers.Conv2D(12, (3, 3), padding='same', activation='softmax')(d4)\n\n    model = Model(inputs=inputs, outputs=outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-11-15T17:01:23.144399Z","iopub.execute_input":"2024-11-15T17:01:23.144865Z","iopub.status.idle":"2024-11-15T17:01:23.155135Z","shell.execute_reply.started":"2024-11-15T17:01:23.144827Z","shell.execute_reply":"2024-11-15T17:01:23.154073Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_shape = (height, width, 3)\nmodel = SegNet(input_shape)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T17:01:25.408522Z","iopub.execute_input":"2024-11-15T17:01:25.409475Z","iopub.status.idle":"2024-11-15T17:01:26.741560Z","shell.execute_reply.started":"2024-11-15T17:01:25.409400Z","shell.execute_reply":"2024-11-15T17:01:26.740617Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-11-15T17:01:28.288279Z","iopub.execute_input":"2024-11-15T17:01:28.289316Z","iopub.status.idle":"2024-11-15T17:01:28.339484Z","shell.execute_reply.started":"2024-11-15T17:01:28.289270Z","shell.execute_reply":"2024-11-15T17:01:28.338570Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"opt = tf.keras.optimizers.Adam(lr)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=[\"sparse_categorical_accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2024-11-15T17:01:30.038780Z","iopub.execute_input":"2024-11-15T17:01:30.039477Z","iopub.status.idle":"2024-11-15T17:01:30.049677Z","shell.execute_reply.started":"2024-11-15T17:01:30.039406Z","shell.execute_reply":"2024-11-15T17:01:30.048664Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"callbacks = [\n    ModelCheckpoint(model_file, verbose=1, save_best_only=True),\n    ReduceLROnPlateau(monitor=\"val_loss\", mode='auto', factor=0.1, patience=4),\n    CSVLogger(log_file),\n    EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n]","metadata":{"execution":{"iopub.status.busy":"2024-11-15T17:01:33.208185Z","iopub.execute_input":"2024-11-15T17:01:33.209055Z","iopub.status.idle":"2024-11-15T17:01:33.214765Z","shell.execute_reply.started":"2024-11-15T17:01:33.209013Z","shell.execute_reply":"2024-11-15T17:01:33.213517Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = model.fit(\n    train_dataset,\n    validation_data=valid_dataset,\n    epochs=epochs,\n    callbacks=callbacks,\n    verbose=1,\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T17:01:34.609323Z","iopub.execute_input":"2024-11-15T17:01:34.610252Z","iopub.status.idle":"2024-11-15T19:28:26.493656Z","shell.execute_reply.started":"2024-11-15T17:01:34.610197Z","shell.execute_reply":"2024-11-15T19:28:26.492734Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tf.keras.config.enable_unsafe_deserialization()\ncustom_objects = {\n    \"BAM\": BAM\n}\nmodel = tf.keras.models.load_model(model_file, custom_objects=custom_objects)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T19:31:09.252147Z","iopub.execute_input":"2024-11-15T19:31:09.253029Z","iopub.status.idle":"2024-11-15T19:31:13.233044Z","shell.execute_reply.started":"2024-11-15T19:31:09.252987Z","shell.execute_reply":"2024-11-15T19:31:13.231936Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n# Load the CSV log file\nlog_file = os.path.join(files_dir, \"Log-Unet.csv\")\nlog_data = pd.read_csv(log_file)\n# Check available columns in the CSV\nprint(log_data.columns)\n\n# Plot Training and Validation Loss\nplt.figure(figsize=(6, 6))\nplt.plot(log_data['loss'], label='Training Loss')\nplt.plot(log_data['val_loss'], label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\n\n# Save the Loss plot\nloss_plot_file_path = os.path.join(files_dir, 'training_validation_loss.png')\nplt.savefig(loss_plot_file_path)\nplt.show()\n\n# Plot Training and Validation Accuracy\nplt.figure(figsize=(6, 6))\nplt.plot(log_data['sparse_categorical_accuracy'], label='Training Accuracy')\nplt.plot(log_data['val_sparse_categorical_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\n\n# Save the Accuracy plot\naccuracy_plot_file_path = os.path.join(files_dir, 'training_validation_accuracy.png')\nplt.savefig(accuracy_plot_file_path)  # Close the figure to free memory\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-15T19:31:39.412212Z","iopub.execute_input":"2024-11-15T19:31:39.412929Z","iopub.status.idle":"2024-11-15T19:31:40.109537Z","shell.execute_reply.started":"2024-11-15T19:31:39.412884Z","shell.execute_reply":"2024-11-15T19:31:40.108591Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_to_rgb = {v: k for k, v in color_map.items()}\n\nclass_colors = {k: tuple(v/255.0 for v in rgb) for k, rgb in class_to_rgb.items()}\ncolors = np.array([class_colors[i] for i in sorted(class_colors.keys())])\ncmap = mcolors.ListedColormap(colors)\nnorm = mcolors.BoundaryNorm(boundaries=np.arange(len(class_colors)+1) - 0.5, ncolors=len(class_colors))\n\ndef map_class_to_rgb(class_mask):\n    rgb_mask = np.zeros((class_mask.shape[0], class_mask.shape[1], 3), dtype=np.uint8)\n    for class_index, rgb in class_to_rgb.items():\n        rgb_mask[class_mask == class_index] = rgb\n    return rgb_mask\n\n\nplt.figure(figsize=(15, 10))  \n\nbatch = next(iter(test_dataset)) \nbatch_x, batch_y = batch\n\nnum_images = batch_x.shape[0]\n\nfor i in range(num_images):\n\n    image = batch_x[i].numpy()\n    mask = batch_y[i].numpy()\n\n\n    prediction = model.predict(np.expand_dims(image, axis=0))[0]  \n    predicted_class_indices = np.argmax(prediction, axis=-1) \n\n    predicted_mask_rgb = map_class_to_rgb(predicted_class_indices)\n\n    original_label_path = test_y[i] \n    original_label = cv2.imread(original_label_path, cv2.IMREAD_COLOR)\n    original_label = cv2.cvtColor(original_label, cv2.COLOR_BGR2RGB)\n    original_label = cv2.resize(original_label, (width, height)) / 255.0\n\n    plt.subplot(num_images, 3, 3*i + 1)\n    plt.imshow(image)\n    plt.title(f\"Input Image {i+1}\")\n    plt.axis(\"off\")\n\n    plt.subplot(num_images, 3, 3*i + 2)\n    plt.imshow(original_label)\n    plt.title(f\"Original Label {i+1}\")\n    plt.axis(\"off\")\n\n    plt.subplot(num_images, 3, 3*i + 3)\n    plt.imshow(predicted_mask_rgb)\n    plt.title(f\"Predicted Mask {i+1}\")\n    plt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-15T19:31:41.312207Z","iopub.execute_input":"2024-11-15T19:31:41.312958Z","iopub.status.idle":"2024-11-15T19:31:50.780523Z","shell.execute_reply.started":"2024-11-15T19:31:41.312917Z","shell.execute_reply":"2024-11-15T19:31:50.779490Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# colors = [\n#     (0, 0, 0),           # Background\n#     (192, 128, 128),     # Person\n#     (0, 128, 0),         # Bike\n#     (128, 128, 128),     # Car\n#     (128, 0, 0),         # Drone\n#     (0, 0, 128),         # Boat\n#     (192, 0, 128),       # Animal\n#     (192, 0, 0),         # Obstacle\n#     (192, 128, 0),       # Construction\n#     (0, 64, 0),          # Vegetation\n#     (128, 128, 0),       # Road\n#     (0, 128, 128),       # Sky\n# ]\n\ntime_taken = []\nfor x in tqdm(test_x):\n    \n    seq_folder = x.split(\"/\")[-3]\n    image_name = x.split(\"/\")[-1]\n    \n    x = cv2.imread(x, cv2.IMREAD_COLOR)\n    x = cv2.resize(x, (width, height))\n    x = x / 255.0\n    x = np.expand_dims(x, axis=0)\n\n    start_time = time.time()\n    p = model.predict(x)[0] \n    total_time = time.time() - start_time\n    time_taken.append(total_time)\n\n    p_class_indices = np.argmax(p, axis=-1)  \n    \n    p_rgb = np.zeros((p_class_indices.shape[0], p_class_indices.shape[1], 3), dtype=np.uint8)\n    \n    for rgb, idx in color_map.items():\n        p_rgb[p_class_indices == idx] = rgb \n    \n    p_rgb = cv2.cvtColor(p_rgb, cv2.COLOR_RGB2BGR)\n\n    save_path_with_name = os.path.join(save_path, f\"{seq_folder}_{image_name}\")\n    cv2.imwrite(save_path_with_name, p_rgb)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T19:32:08.033190Z","iopub.execute_input":"2024-11-15T19:32:08.033883Z","iopub.status.idle":"2024-11-15T19:32:08.227348Z","shell.execute_reply.started":"2024-11-15T19:32:08.033840Z","shell.execute_reply":"2024-11-15T19:32:08.226043Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r BAMsegnetAEROSCAPES.zip /kaggle/working\nfrom IPython.display import FileLink\nFileLink(r'BAMsegnetAEROSCAPES.zip')","metadata":{"execution":{"iopub.status.busy":"2024-11-15T19:32:10.192703Z","iopub.execute_input":"2024-11-15T19:32:10.193743Z","iopub.status.idle":"2024-11-15T19:32:15.350481Z","shell.execute_reply.started":"2024-11-15T19:32:10.193695Z","shell.execute_reply":"2024-11-15T19:32:15.349201Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}