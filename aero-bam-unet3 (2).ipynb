{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9924187,"sourceType":"datasetVersion","datasetId":6099616}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom glob import glob\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Reshape, Dense, multiply\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications import ResNet50\nfrom sklearn.model_selection import train_test_split\nimport cv2\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nfrom tensorflow.keras import layers\nfrom tqdm import tqdm\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-16T13:35:44.308610Z","iopub.execute_input":"2024-11-16T13:35:44.309002Z","iopub.status.idle":"2024-11-16T13:35:44.315644Z","shell.execute_reply.started":"2024-11-16T13:35:44.308965Z","shell.execute_reply":"2024-11-16T13:35:44.314731Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 6\nlr = 1e-3\nepochs = 200\nwidth = 256\nheight = 256","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T13:35:45.782283Z","iopub.execute_input":"2024-11-16T13:35:45.782923Z","iopub.status.idle":"2024-11-16T13:35:45.787367Z","shell.execute_reply.started":"2024-11-16T13:35:45.782882Z","shell.execute_reply":"2024-11-16T13:35:45.786380Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_path = os.path.join(\"/kaggle/input/aeroscapes1/aeroscapes\")\nfiles_dir = os.path.join(\"files\", \"modified_uavid_dataset\")\nmodel_file = os.path.join(files_dir, \"UnetModel.keras\")\nlog_file = os.path.join(files_dir, \"Log-Unet.csv\")\n\n# Function to create directory\ndef create_dir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n        \ncreate_dir(files_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T13:35:47.222265Z","iopub.execute_input":"2024-11-16T13:35:47.223049Z","iopub.status.idle":"2024-11-16T13:35:47.228653Z","shell.execute_reply.started":"2024-11-16T13:35:47.223007Z","shell.execute_reply":"2024-11-16T13:35:47.227661Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_data(path):\n    Images = sorted(glob(os.path.join(path, \"JPEGImages\", \"*\")))\n    Labels = sorted(glob(os.path.join(path, \"Visualizations\", \"*\")))\n\n    return (Images, Labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T13:35:48.502903Z","iopub.execute_input":"2024-11-16T13:35:48.503320Z","iopub.status.idle":"2024-11-16T13:35:48.508647Z","shell.execute_reply.started":"2024-11-16T13:35:48.503280Z","shell.execute_reply":"2024-11-16T13:35:48.507700Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"(Images, Labels) = load_data(dataset_path)\n\nprint(f\"New Train: {len(Images)} - {len(Labels)}\")\n\n# First, split off 10% of the data for testing\ntrain_val_images, test_x, train_val_labels, test_y = train_test_split(Images, Labels, test_size=0.1, random_state=42)\n\n# Then, split the remaining 90% into 70% training and 20% validation (0.7 / 0.9 â‰ˆ 0.78)\ntrain_x, valid_x, train_y, valid_y = train_test_split(train_val_images, train_val_labels, test_size=0.22, random_state=42)\n\nprint(f\"Training set: {len(train_x)} images\")\nprint(f\"Validation set: {len(valid_x)} images\")\nprint(f\"Test set: {len(test_x)} images\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T13:35:49.853024Z","iopub.execute_input":"2024-11-16T13:35:49.853930Z","iopub.status.idle":"2024-11-16T13:35:49.892787Z","shell.execute_reply.started":"2024-11-16T13:35:49.853887Z","shell.execute_reply":"2024-11-16T13:35:49.891760Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def read_image(path):\n    path = path.decode()\n    x = cv2.imread(path, cv2.IMREAD_COLOR)\n    x = cv2.resize(x, (width, height))\n    x = x / 255.0\n    x = x.astype(np.float32)\n    return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T13:35:51.212710Z","iopub.execute_input":"2024-11-16T13:35:51.213514Z","iopub.status.idle":"2024-11-16T13:35:51.219071Z","shell.execute_reply.started":"2024-11-16T13:35:51.213470Z","shell.execute_reply":"2024-11-16T13:35:51.217737Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"color_map = {\n    (0, 0, 0): 0,            # Background\n    (192, 128, 128): 1,      # Person\n    (0, 128, 0): 2,          # Bike\n    (128, 128, 128): 3,      # Car\n    (128, 0, 0): 4,          # Drone\n    (0, 0, 128): 5,          # Boat\n    (192, 0, 128): 6,        # Animal\n    (192, 0, 0): 7,          # Obstacle\n    (192, 128, 0): 8,        # Construction\n    (0, 64, 0): 9,           # Vegetation\n    (128, 128, 0): 10,       # Road\n    (0, 128, 128): 11,       # Sky\n}\n\ndef read_mask(path):\n    path = path.decode()\n    mask = cv2.imread(path)  \n    mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB) \n    mask = cv2.resize(mask, (width, height), interpolation=cv2.INTER_NEAREST)\n\n    class_indices = np.zeros((height, width), dtype=np.uint8)\n\n    for rgb, idx in color_map.items():\n        class_indices[(mask == rgb).all(axis=-1)] = idx\n\n    return class_indices.astype(np.uint8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T13:35:52.662121Z","iopub.execute_input":"2024-11-16T13:35:52.662532Z","iopub.status.idle":"2024-11-16T13:35:52.671997Z","shell.execute_reply.started":"2024-11-16T13:35:52.662492Z","shell.execute_reply":"2024-11-16T13:35:52.671064Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tf_parse(x, y):\n    def _parse(x, y):\n        x = read_image(x) \n        y = read_mask(y) \n        return x, y\n    \n    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.uint8]) \n    x.set_shape([height, width, 3]) \n    y.set_shape([height, width])   \n    return x, y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T13:35:54.063460Z","iopub.execute_input":"2024-11-16T13:35:54.063862Z","iopub.status.idle":"2024-11-16T13:35:54.072233Z","shell.execute_reply.started":"2024-11-16T13:35:54.063825Z","shell.execute_reply":"2024-11-16T13:35:54.071262Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tf_dataset(x, y, batch=6):\n    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n    dataset = dataset.map(tf_parse, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.batch(batch)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T13:35:55.387032Z","iopub.execute_input":"2024-11-16T13:35:55.387441Z","iopub.status.idle":"2024-11-16T13:35:55.392963Z","shell.execute_reply.started":"2024-11-16T13:35:55.387402Z","shell.execute_reply":"2024-11-16T13:35:55.392021Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = tf_dataset(train_x, train_y, batch=batch_size)\nvalid_dataset = tf_dataset(valid_x, valid_y, batch=batch_size)\ntest_dataset = tf_dataset(test_x, test_y, batch=batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T13:35:56.662467Z","iopub.execute_input":"2024-11-16T13:35:56.662877Z","iopub.status.idle":"2024-11-16T13:35:56.817630Z","shell.execute_reply.started":"2024-11-16T13:35:56.662839Z","shell.execute_reply":"2024-11-16T13:35:56.816592Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a custom color map based on the color_map dictionary\ncolor_map_values = list(color_map.keys())\ncolor_map_rgb = np.array(color_map_values) / 255.0  # Normalize to 0-1 range for matplotlib\n\ndef plot_samples_with_labels(dataset, title):\n    plt.figure(figsize=(12, 12))\n    for i, (images, masks) in enumerate(dataset.take(1)):  # Take a single batch\n        for j in range(4):  # Loop over first four images\n            # Plot the image\n            plt.subplot(4, 4, j*2 + 1)\n            plt.imshow(images[j])\n            plt.axis(\"off\")\n            plt.title(f\"{['Train', 'Validation', 'Test'][title]} Image {j+1}\")\n\n            # Convert class indices in the mask to RGB colors\n            mask_rgb = np.zeros((height, width, 3), dtype=np.float32)\n            for idx, color in enumerate(color_map_rgb):\n                mask_rgb[masks[j] == idx] = color\n\n            # Plot the label mask\n            plt.subplot(4, 4, j*2 + 2)\n            plt.imshow(mask_rgb)\n            plt.axis(\"off\")\n            plt.title(f\"{['Train', 'Validation', 'Test'][title]} Mask {j+1}\")\n    \n    plt.tight_layout()\n    plt.show()\n\n# Plot the images and corresponding label masks for each dataset\nplot_samples_with_labels(train_dataset, title=0)   # Train images and masks\nplot_samples_with_labels(valid_dataset, title=1)   # Validation images and masks\nplot_samples_with_labels(test_dataset, title=2)    # Test images and masks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T13:35:58.102755Z","iopub.execute_input":"2024-11-16T13:35:58.103163Z","iopub.status.idle":"2024-11-16T13:36:01.285168Z","shell.execute_reply.started":"2024-11-16T13:35:58.103123Z","shell.execute_reply":"2024-11-16T13:36:01.284244Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@tf.keras.utils.register_keras_serializable(package='Custom', name='BAM')\nclass BAM(tf.keras.layers.Layer):\n    def __init__(self, channel, reduction=16, dilation_conv_num=2, **kwargs):  # Accept arbitrary kwargs\n        super(BAM, self).__init__(**kwargs)\n        \n        # Channel Attention Module\n        self.channel_avg_pool = layers.GlobalAveragePooling2D()\n        self.channel_fc1 = layers.Dense(channel // reduction, activation='relu')\n        self.channel_fc2 = layers.Dense(channel)\n        \n        # Spatial Attention Module\n        self.spatial_conv1 = layers.Conv2D(channel // reduction, kernel_size=1, activation='relu')\n        self.spatial_dilated_convs = [\n            layers.Conv2D(channel // reduction, kernel_size=3, padding='same', dilation_rate=dilation, activation='relu')\n            for dilation in range(1, dilation_conv_num + 1)\n        ]\n        self.spatial_conv2 = layers.Conv2D(1, kernel_size=1)\n\n    def call(self, x):\n        # Channel Attention\n        channel_attention = self.channel_avg_pool(x)\n        channel_attention = self.channel_fc1(channel_attention)\n        channel_attention = self.channel_fc2(channel_attention)\n        channel_attention = tf.nn.sigmoid(channel_attention)\n        channel_attention = tf.reshape(channel_attention, [-1, 1, 1, x.shape[-1]])\n        channel_refined = x * channel_attention\n\n        # Spatial Attention\n        spatial_attention = self.spatial_conv1(channel_refined)\n        for conv in self.spatial_dilated_convs:\n            spatial_attention = conv(spatial_attention)\n        spatial_attention = self.spatial_conv2(spatial_attention)\n        spatial_attention = tf.nn.sigmoid(spatial_attention)\n        \n        # Combining Attention\n        refined_feature = x * spatial_attention + x * channel_attention\n        return refined_feature","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T13:36:01.768021Z","iopub.execute_input":"2024-11-16T13:36:01.768838Z","iopub.status.idle":"2024-11-16T13:36:01.780180Z","shell.execute_reply.started":"2024-11-16T13:36:01.768796Z","shell.execute_reply":"2024-11-16T13:36:01.779169Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def conv_block(x, num_filters, act=True, use_bam=False):\n    x = L.SeparableConv2D(num_filters, kernel_size=3, padding=\"same\")(x)\n    x = L.BatchNormalization()(x)\n    x = L.Activation(\"relu\")(x)\n    \n    if act:\n        x = L.Conv2D(num_filters, (3, 3), activation='relu', padding='same')(x)\n        x = L.BatchNormalization()(x)\n        x = L.Activation(\"relu\")(x)\n    \n    if use_bam:\n        x = BAM(channel=num_filters)(x)\n    return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T13:36:03.917658Z","iopub.execute_input":"2024-11-16T13:36:03.918058Z","iopub.status.idle":"2024-11-16T13:36:03.924858Z","shell.execute_reply.started":"2024-11-16T13:36:03.918019Z","shell.execute_reply":"2024-11-16T13:36:03.923870Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def unet_three_plus(input_shape, expansion=1):\n    \"\"\" Inputs \"\"\"\n    inputs = L.Input(input_shape, name=\"input_layer\")  ## (256 x 256 x 3)\n\n    \"\"\" Encoder \"\"\"\n    base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=inputs)\n    e1 = base_model.get_layer('conv1_relu').output\n    e2 = base_model.get_layer('conv2_block3_out').output\n    e3 = base_model.get_layer('conv3_block4_out').output\n    e4 = base_model.get_layer('conv4_block6_out').output\n    e5 = base_model.get_layer('conv5_block3_out').output\n\n    # Apply BAM to encoder outputs\n    # e1 = BAM(channel=e1.shape[-1])(e1)\n    e2 = BAM(channel=e2.shape[-1])(e2)\n    e3 = BAM(channel=e3.shape[-1])(e3)\n    e4 = BAM(channel=e4.shape[-1])(e4)\n    e5 = BAM(channel=e5.shape[-1])(e5)\n\n    \"\"\" Decoder 4 \"\"\"\n    e1_d4 = L.MaxPool2D((8, 8))(e1)\n    e1_d4 = conv_block(e1_d4, 16 * expansion, act=False, use_bam=True)\n\n    e2_d4 = L.MaxPool2D((4, 4))(e2)\n    e2_d4 = conv_block(e2_d4, 16 * expansion, act=False, use_bam=True)\n\n    e3_d4 = L.MaxPool2D((2, 2))(e3)\n    e3_d4 = conv_block(e3_d4, 16 * expansion, act=False, use_bam=True)\n\n    e4_d4 = conv_block(e4, 16 * expansion, act=False, use_bam=True)\n\n    e5_d4 = L.UpSampling2D((2, 2), interpolation=\"bilinear\")(e5)\n    e5_d4 = conv_block(e5_d4, 16 * expansion, act=False, use_bam=True)\n\n    d4 = L.Concatenate()([e1_d4, e2_d4, e3_d4, e4_d4, e5_d4])\n    d4 = conv_block(d4, 16 * 5 * expansion, act=False, use_bam=True)\n\n    \"\"\" Decoder 3 \"\"\"\n    e1_d3 = L.MaxPool2D((4, 4))(e1)\n    e1_d3 = conv_block(e1_d3, 16 * expansion, act=False, use_bam=True)\n\n    e2_d3 = L.MaxPool2D((2, 2))(e2)\n    e2_d3 = conv_block(e2_d3, 16 * expansion, act=False, use_bam=True)\n\n    e3_d3 = conv_block(e3, 16 * expansion, act=False, use_bam=True)\n\n    d4_d3 = L.UpSampling2D((2, 2), interpolation=\"bilinear\")(d4)\n    d4_d3 = conv_block(d4_d3, 16 * expansion, act=False, use_bam=True)\n\n    e5_d3 = L.UpSampling2D((4, 4), interpolation=\"bilinear\")(e5)\n    e5_d3 = conv_block(e5_d3, 16 * expansion, act=False, use_bam=True)\n\n    d3 = L.Concatenate()([e1_d3, e2_d3, e3_d3, d4_d3, e5_d3])\n    d3 = conv_block(d3, 16 * 5 * expansion, act=False, use_bam=True)\n\n    \"\"\" Decoder 2 \"\"\"\n    e1_d2 = L.MaxPool2D((2, 2))(e1)\n    e1_d2 = conv_block(e1_d2, 16 * expansion, act=False, use_bam=True)\n\n    e2_d2 = conv_block(e2, 16 * expansion, act=False, use_bam=True)\n\n    d3_d2 = L.UpSampling2D((2, 2), interpolation=\"bilinear\")(d3)\n    d3_d2 = conv_block(d3_d2, 16 * expansion, act=False, use_bam=True)\n\n    d4_d2 = L.UpSampling2D((4, 4), interpolation=\"bilinear\")(d4)\n    d4_d2 = conv_block(d4_d2, 16 * expansion, act=False, use_bam=True)\n\n    e5_d2 = L.UpSampling2D((8, 8), interpolation=\"bilinear\")(e5)\n    e5_d2 = conv_block(e5_d2, 16 * expansion, act=False, use_bam=True)\n\n    d2 = L.Concatenate()([e1_d2, e2_d2, d3_d2, d4_d2, e5_d2])\n    d2 = conv_block(d2, 16 * 5 * expansion, act=False, use_bam=True)\n\n    \"\"\" Decoder 1 \"\"\"\n    e1_d1 = conv_block(e1, 16 * expansion, act=False, use_bam=True)\n\n    d2_d1 = L.UpSampling2D((2, 2), interpolation=\"bilinear\")(d2)\n    d2_d1 = conv_block(d2_d1, 16 * expansion, act=False, use_bam=True)\n\n    d3_d1 = L.UpSampling2D((4, 4), interpolation=\"bilinear\")(d3)\n    d3_d1 = conv_block(d3_d1, 16 * expansion, act=False, use_bam=True)\n\n    d4_d1 = L.UpSampling2D((8, 8), interpolation=\"bilinear\")(d4)\n    d4_d1 = conv_block(d4_d1, 16 * expansion, act=False, use_bam=True)\n\n    e5_d1 = L.UpSampling2D((16, 16), interpolation=\"bilinear\")(e5)\n    e5_d1 = conv_block(e5_d1, 16 * expansion, act=False, use_bam=True)\n\n    d1 = L.Concatenate()([e1_d1, d2_d1, d3_d1, d4_d1, e5_d1])\n    d1 = conv_block(d1, 16 * 5 * expansion, act=False, use_bam=True)\n    d1 = L.UpSampling2D((2, 2), interpolation=\"bilinear\")(d1)\n\n    \"\"\" Output \"\"\"\n    y1 = L.Conv2D(12, kernel_size=1, padding=\"same\")(d1)\n    y1 = L.Activation(\"softmax\")(y1)\n\n    outputs = [y1]\n\n    model = tf.keras.Model(inputs, outputs, name=\"Unet3PlusBAM\")\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T13:36:05.528332Z","iopub.execute_input":"2024-11-16T13:36:05.528696Z","iopub.status.idle":"2024-11-16T13:36:05.553083Z","shell.execute_reply.started":"2024-11-16T13:36:05.528662Z","shell.execute_reply":"2024-11-16T13:36:05.552024Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_shape = (height, width, 3)\nnum_classes = 12\n\n# Instantiate the model\nmodel = unet_three_plus(input_shape, num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T13:36:07.282928Z","iopub.execute_input":"2024-11-16T13:36:07.283828Z","iopub.status.idle":"2024-11-16T13:36:14.852778Z","shell.execute_reply.started":"2024-11-16T13:36:07.283784Z","shell.execute_reply":"2024-11-16T13:36:14.851937Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T13:36:16.822204Z","iopub.execute_input":"2024-11-16T13:36:16.822613Z","iopub.status.idle":"2024-11-16T13:36:17.211553Z","shell.execute_reply.started":"2024-11-16T13:36:16.822572Z","shell.execute_reply":"2024-11-16T13:36:17.210512Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"opt = tf.keras.optimizers.Adam(lr)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=[\"sparse_categorical_accuracy\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T13:36:53.062795Z","iopub.execute_input":"2024-11-16T13:36:53.063193Z","iopub.status.idle":"2024-11-16T13:36:53.074151Z","shell.execute_reply.started":"2024-11-16T13:36:53.063153Z","shell.execute_reply":"2024-11-16T13:36:53.073230Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"callbacks = [\n    ModelCheckpoint(model_file, verbose=1, save_best_only=True),\n    ReduceLROnPlateau(monitor=\"val_loss\", mode='auto', factor=0.1, patience=4),\n    CSVLogger(log_file),\n    EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T13:36:54.662419Z","iopub.execute_input":"2024-11-16T13:36:54.662821Z","iopub.status.idle":"2024-11-16T13:36:54.668441Z","shell.execute_reply.started":"2024-11-16T13:36:54.662774Z","shell.execute_reply":"2024-11-16T13:36:54.667297Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = model.fit(\n    train_dataset,\n    validation_data=valid_dataset,\n    epochs=epochs,\n    callbacks=callbacks,\n    verbose=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T13:36:56.203659Z","iopub.execute_input":"2024-11-16T13:36:56.204300Z","iopub.status.idle":"2024-11-16T17:36:43.721967Z","shell.execute_reply.started":"2024-11-16T13:36:56.204255Z","shell.execute_reply":"2024-11-16T17:36:43.720925Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"custom_objects = {\n    \"custom_BAM\": BAM,\n}\nmodel = tf.keras.models.load_model(model_file, custom_objects=custom_objects)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T17:47:12.596698Z","iopub.execute_input":"2024-11-16T17:47:12.597839Z","iopub.status.idle":"2024-11-16T17:48:30.477296Z","shell.execute_reply.started":"2024-11-16T17:47:12.597777Z","shell.execute_reply":"2024-11-16T17:48:30.476279Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport os\n# Load the CSV log file\nlog_file = os.path.join(files_dir, \"Log-Unet.csv\")\nlog_data = pd.read_csv(log_file)\n\n# Check available columns in the CSV\nprint(log_data.columns)\n\n# Plot Training and Validation Loss\nplt.figure(figsize=(6, 6))\nplt.plot(log_data['loss'], label='Training Loss')\nplt.plot(log_data['val_loss'], label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\n# Save the Loss plot\nloss_plot_file_path = os.path.join(files_dir, 'training_validation_loss.png')\nplt.savefig(loss_plot_file_path) # Close the figure to free memory\nplt.show()\n\n# Plot Training and Validation Accuracy\nplt.figure(figsize=(6, 6))\nplt.plot(log_data['sparse_categorical_accuracy'], label='Training Accuracy')\nplt.plot(log_data['val_sparse_categorical_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\n# Save the Accuracy plot\naccuracy_plot_file_path = os.path.join(files_dir, 'training_validation_accuracy.png')\nplt.savefig(accuracy_plot_file_path)  # Close the figure to free memory\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T17:48:43.281720Z","iopub.execute_input":"2024-11-16T17:48:43.282107Z","iopub.status.idle":"2024-11-16T17:48:43.960332Z","shell.execute_reply.started":"2024-11-16T17:48:43.282068Z","shell.execute_reply":"2024-11-16T17:48:43.959366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_to_rgb = {v: k for k, v in color_map.items()}\n\nclass_colors = {k: tuple(v/255.0 for v in rgb) for k, rgb in class_to_rgb.items()}\ncolors = np.array([class_colors[i] for i in sorted(class_colors.keys())])\ncmap = mcolors.ListedColormap(colors)\nnorm = mcolors.BoundaryNorm(boundaries=np.arange(len(class_colors)+1) - 0.5, ncolors=len(class_colors))\n\ndef map_class_to_rgb(class_mask):\n    rgb_mask = np.zeros((class_mask.shape[0], class_mask.shape[1], 3), dtype=np.uint8)\n    for class_index, rgb in class_to_rgb.items():\n        rgb_mask[class_mask == class_index] = rgb\n    return rgb_mask\n\n\nplt.figure(figsize=(15, 10))  \n\nbatch = next(iter(test_dataset)) \nbatch_x, batch_y = batch\n\nnum_images = batch_x.shape[0]\n\nfor i in range(num_images):\n\n    image = batch_x[i].numpy()\n    mask = batch_y[i].numpy()\n\n\n    prediction = model.predict(np.expand_dims(image, axis=0))[0]  \n    predicted_class_indices = np.argmax(prediction, axis=-1) \n\n    predicted_mask_rgb = map_class_to_rgb(predicted_class_indices)\n\n    original_label_path = test_y[i] \n    original_label = cv2.imread(original_label_path, cv2.IMREAD_COLOR)\n    original_label = cv2.cvtColor(original_label, cv2.COLOR_BGR2RGB)\n    original_label = cv2.resize(original_label, (width, height)) / 255.0\n\n    plt.subplot(num_images, 3, 3*i + 1)\n    plt.imshow(image)\n    plt.title(f\"Input Image {i+1}\")\n    plt.axis(\"off\")\n\n    plt.subplot(num_images, 3, 3*i + 2)\n    plt.imshow(original_label)\n    plt.title(f\"Original Label {i+1}\")\n    plt.axis(\"off\")\n\n    plt.subplot(num_images, 3, 3*i + 3)\n    plt.imshow(predicted_mask_rgb)\n    plt.title(f\"Predicted Mask {i+1}\")\n    plt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T17:48:45.760586Z","iopub.execute_input":"2024-11-16T17:48:45.760969Z","iopub.status.idle":"2024-11-16T17:49:02.573283Z","shell.execute_reply.started":"2024-11-16T17:48:45.760931Z","shell.execute_reply":"2024-11-16T17:49:02.572369Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n\n# colors = [\n#     (0, 0, 0),           # Background\n#     (192, 128, 128),     # Person\n#     (0, 128, 0),         # Bike\n#     (128, 128, 128),     # Car\n#     (128, 0, 0),         # Drone\n#     (0, 0, 128),         # Boat\n#     (192, 0, 128),       # Animal\n#     (192, 0, 0),         # Obstacle\n#     (192, 128, 0),       # Construction\n#     (0, 64, 0),          # Vegetation\n#     (128, 128, 0),       # Road\n#     (0, 128, 128),       # Sky\n# ]\n\ntime_taken = []\nfor x in tqdm(test_x):\n    \n    seq_folder = x.split(\"/\")[-3]\n    image_name = x.split(\"/\")[-1]\n    \n    x = cv2.imread(x, cv2.IMREAD_COLOR)\n    x = cv2.resize(x, (width, height))\n    x = x / 255.0\n    x = np.expand_dims(x, axis=0)\n\n    start_time = time.time()\n    p = model.predict(x)[0] \n    total_time = time.time() - start_time\n    time_taken.append(total_time)\n\n    p_class_indices = np.argmax(p, axis=-1)  \n    \n    p_rgb = np.zeros((p_class_indices.shape[0], p_class_indices.shape[1], 3), dtype=np.uint8)\n    \n    for rgb, idx in color_map.items():\n        p_rgb[p_class_indices == idx] = rgb \n    \n    p_rgb = cv2.cvtColor(p_rgb, cv2.COLOR_RGB2BGR)\n\n    save_path_with_name = os.path.join(save_path, f\"{seq_folder}_{image_name}\")\n    cv2.imwrite(save_path_with_name, p_rgb)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T17:51:25.206078Z","iopub.execute_input":"2024-11-16T17:51:25.206920Z","iopub.status.idle":"2024-11-16T17:51:25.411608Z","shell.execute_reply.started":"2024-11-16T17:51:25.206881Z","shell.execute_reply":"2024-11-16T17:51:25.410470Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r BAMunet3plusAERO.zip /kaggle/working\nfrom IPython.display import FileLink\nFileLink(r'BAMunet3plusAERO.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T17:51:31.960943Z","iopub.execute_input":"2024-11-16T17:51:31.961333Z","iopub.status.idle":"2024-11-16T17:51:53.270688Z","shell.execute_reply.started":"2024-11-16T17:51:31.961295Z","shell.execute_reply":"2024-11-16T17:51:53.269540Z"}},"outputs":[],"execution_count":null}]}