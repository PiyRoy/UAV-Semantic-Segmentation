{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2125129,"sourceType":"datasetVersion","datasetId":1077128}],"dockerImageVersionId":30761,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\nimport tensorflow as tf\nfrom glob import glob\nimport cv2\nfrom keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\nfrom tensorflow.keras import layers\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\nfrom keras.models import Model\nimport tensorflow.keras.backend as K\nfrom keras.layers import Cropping2D, Dropout\nfrom keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Dense, multiply, Permute, Add, Activation, Lambda\nfrom tqdm import tqdm\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-10-28T14:02:52.964743Z","iopub.execute_input":"2024-10-28T14:02:52.965867Z","iopub.status.idle":"2024-10-28T14:02:52.972590Z","shell.execute_reply.started":"2024-10-28T14:02:52.965825Z","shell.execute_reply":"2024-10-28T14:02:52.971624Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set parameters\nbatch_size = 5\nlr = 1e-3\nepochs = 100\nwidth = 512\nheight = 512","metadata":{"execution":{"iopub.status.busy":"2024-10-28T14:02:53.889202Z","iopub.execute_input":"2024-10-28T14:02:53.889575Z","iopub.status.idle":"2024-10-28T14:02:53.894214Z","shell.execute_reply.started":"2024-10-28T14:02:53.889539Z","shell.execute_reply":"2024-10-28T14:02:53.893207Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_path = os.path.join(\"/kaggle/input/uavid-v1\")\nfiles_dir = os.path.join(\"files\", \"modified_uavid_dataset\")\nmodel_file = os.path.join(files_dir, \"UnetModel.keras\")\nlog_file = os.path.join(files_dir, \"Log-Unet.csv\")\n\n# Function to create directory\ndef create_dir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n        \ncreate_dir(files_dir)","metadata":{"execution":{"iopub.status.busy":"2024-10-28T14:02:54.764461Z","iopub.execute_input":"2024-10-28T14:02:54.765365Z","iopub.status.idle":"2024-10-28T14:02:54.771673Z","shell.execute_reply.started":"2024-10-28T14:02:54.765321Z","shell.execute_reply":"2024-10-28T14:02:54.770599Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_data(path):\n    train_x = sorted(glob(os.path.join(path, \"uavid_train\",\"seq1\", \"Images\", \"*\")))\n    train_y = sorted(glob(os.path.join(path, \"uavid_train\",\"seq1\", \"Labels\", \"*\")))\n\n    valid_x = sorted(glob(os.path.join(path, \"uavid_val\", \"seq16\", \"Images\", \"*\")))\n    valid_y = sorted(glob(os.path.join(path, \"uavid_val\", \"seq16\", \"Labels\", \"*\")))\n\n    return (train_x, train_y), (valid_x, valid_y)","metadata":{"execution":{"iopub.status.busy":"2024-10-28T14:02:55.904407Z","iopub.execute_input":"2024-10-28T14:02:55.905059Z","iopub.status.idle":"2024-10-28T14:02:55.911175Z","shell.execute_reply.started":"2024-10-28T14:02:55.905013Z","shell.execute_reply":"2024-10-28T14:02:55.910140Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def read_image(path):\n    path = path.decode()\n    x = cv2.imread(path, cv2.IMREAD_COLOR)\n    x = cv2.resize(x, (width, height))\n    x = x / 255.0\n    x = x.astype(np.float32)\n    return x","metadata":{"execution":{"iopub.status.busy":"2024-10-28T14:02:56.869041Z","iopub.execute_input":"2024-10-28T14:02:56.869716Z","iopub.status.idle":"2024-10-28T14:02:56.874664Z","shell.execute_reply.started":"2024-10-28T14:02:56.869664Z","shell.execute_reply":"2024-10-28T14:02:56.873738Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"color_map = {\n    (0, 0, 0): 0,         # Background clutter\n    (128, 0, 0): 1,       # Building\n    (128, 64, 128): 2,    # Road\n    (0, 128, 0): 3,       # Tree\n    (128, 128, 0): 4,     # Low vegetation\n    (64, 0, 128): 5,      # Moving car\n    (192, 0, 192): 6,     # Static car\n    (64, 64, 0): 7        # Human\n}\n\ndef read_mask(path):\n    path = path.decode()\n    mask = cv2.imread(path)  # Read as a color image (BGR format)\n    mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n    mask = cv2.resize(mask, (width, height), interpolation=cv2.INTER_NEAREST)\n\n    # Create a blank mask to hold class indices\n    class_indices = np.zeros((height, width), dtype=np.uint8)\n\n    # Map each RGB value to the corresponding class index\n    for rgb, idx in color_map.items():\n        class_indices[(mask == rgb).all(axis=-1)] = idx\n\n    return class_indices.astype(np.uint8) ","metadata":{"execution":{"iopub.status.busy":"2024-10-28T14:02:59.064904Z","iopub.execute_input":"2024-10-28T14:02:59.065294Z","iopub.status.idle":"2024-10-28T14:02:59.073597Z","shell.execute_reply.started":"2024-10-28T14:02:59.065259Z","shell.execute_reply":"2024-10-28T14:02:59.072566Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tf_parse(x, y):\n    def _parse(x, y):\n        x = read_image(x)  # Image as float32\n        y = read_mask(y)   # Mask as class indices\n        return x, y\n    \n    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.uint8]) \n    x.set_shape([height, width, 3]) \n    y.set_shape([height, width])   \n    return x, y","metadata":{"execution":{"iopub.status.busy":"2024-10-28T14:03:00.464620Z","iopub.execute_input":"2024-10-28T14:03:00.465030Z","iopub.status.idle":"2024-10-28T14:03:00.471028Z","shell.execute_reply.started":"2024-10-28T14:03:00.464994Z","shell.execute_reply":"2024-10-28T14:03:00.470014Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tf_dataset(x, y, batch=10):\n    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n    dataset = dataset.map(tf_parse, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.batch(batch)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2024-10-28T14:03:01.499748Z","iopub.execute_input":"2024-10-28T14:03:01.500144Z","iopub.status.idle":"2024-10-28T14:03:01.505887Z","shell.execute_reply.started":"2024-10-28T14:03:01.500109Z","shell.execute_reply":"2024-10-28T14:03:01.504824Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Load dataset\n(train_x, train_y), (valid_x, valid_y) = load_data(dataset_path)\n\nprint(f\"Train: {len(train_x)} - {len(train_y)}\")\nprint(f\"Valid: {len(valid_x)} - {len(valid_y)}\")\n\n# Split training set to get 500 for training and 100 for validation\ntrain_x, additional_valid_x, train_y, additional_valid_y = train_test_split(\n    train_x, train_y, test_size=100, random_state=42\n)\n\n# Combine 100 images from the training set with the existing validation set\nvalid_x = np.concatenate([valid_x, additional_valid_x], axis=0)\nvalid_y = np.concatenate([valid_y, additional_valid_y], axis=0)\n\n\nprint(f\"New Train: {len(train_x)} - {len(train_y)}\")\nprint(f\"New Valid: {len(valid_x)} - {len(valid_y)}\")\n\n\ntrain_dataset = tf_dataset(train_x, train_y, batch=batch_size)\nvalid_dataset = tf_dataset(valid_x, valid_y, batch=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-10-28T14:03:02.669737Z","iopub.execute_input":"2024-10-28T14:03:02.670515Z","iopub.status.idle":"2024-10-28T14:03:04.254215Z","shell.execute_reply.started":"2024-10-28T14:03:02.670475Z","shell.execute_reply":"2024-10-28T14:03:04.253380Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Channel Attention Function\ndef channel_attention(x, channel, reduction=16):\n    # Global Average Pooling\n    avg_pool = layers.GlobalAveragePooling2D()(x)\n    fc1 = layers.Dense(channel // reduction, activation='relu')(avg_pool)\n    fc2 = layers.Dense(channel)(fc1)\n    attention = Activation('sigmoid')(fc2)\n    attention = layers.Reshape((1, 1, channel))(attention)  # Use Keras Reshape\n    refined = x * attention\n    return refined\n\n# Spatial Attention Function\ndef spatial_attention(x, channel, reduction=16, dilation_conv_num=2):\n    spatial = layers.Conv2D(channel // reduction, kernel_size=1, activation='relu')(x)\n    for dilation in range(1, dilation_conv_num + 1):\n        spatial = layers.Conv2D(channel // reduction, kernel_size=3, padding='same', dilation_rate=dilation, activation='relu')(spatial)\n    spatial = layers.Conv2D(1, kernel_size=1)(spatial)\n    attention = Activation('sigmoid')(spatial)\n    refined = x * attention\n    return refined\n\n# BAM Function: Combine Channel and Spatial Attention\ndef BAM(x, reduction=16, dilation_conv_num=2):\n    channel = x.shape[-1]\n    # Apply Channel Attention\n    channel_refined = channel_attention(x, channel, reduction)\n    \n    # Apply Spatial Attention on the result of Channel Attention\n    spatial_refined = spatial_attention(channel_refined, channel, reduction, dilation_conv_num)\n    \n    # Combine the attention features: input * spatial_attention + input * channel_attention\n    refined_feature = x * spatial_refined + x * channel_refined\n    return refined_feature\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-28T14:03:05.339965Z","iopub.execute_input":"2024-10-28T14:03:05.340757Z","iopub.status.idle":"2024-10-28T14:03:05.350455Z","shell.execute_reply.started":"2024-10-28T14:03:05.340717Z","shell.execute_reply":"2024-10-28T14:03:05.349534Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def conv_block(inputs, num_filters):\n    x = Conv2D(num_filters, (3, 3), padding=\"same\")(inputs)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = Conv2D(num_filters, (3, 3), padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    return x","metadata":{"execution":{"iopub.status.busy":"2024-10-28T14:03:07.704837Z","iopub.execute_input":"2024-10-28T14:03:07.705481Z","iopub.status.idle":"2024-10-28T14:03:07.711103Z","shell.execute_reply.started":"2024-10-28T14:03:07.705441Z","shell.execute_reply":"2024-10-28T14:03:07.710132Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def encoder_block(inputs, num_filters):\n    x = conv_block(inputs, num_filters)\n    x = BAM(x, num_filters)  # Apply BAM\n    p = MaxPool2D((2, 2))(x)\n    return x, p","metadata":{"execution":{"iopub.status.busy":"2024-10-28T14:03:09.704872Z","iopub.execute_input":"2024-10-28T14:03:09.705272Z","iopub.status.idle":"2024-10-28T14:03:09.710647Z","shell.execute_reply.started":"2024-10-28T14:03:09.705235Z","shell.execute_reply":"2024-10-28T14:03:09.709471Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def decoder_block(inputs, skip, num_filters):\n    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(inputs)\n    \n    crop_height = max(0, x.shape[1] - skip.shape[1])\n    crop_width = max(0, x.shape[2] - skip.shape[2])\n    if crop_height > 0 or crop_width > 0:\n        skip = Cropping2D(((crop_height // 2, crop_height // 2), (crop_width // 2, crop_width // 2)))(skip)\n    \n    x = Concatenate()([x, skip])\n    x = conv_block(x, num_filters)\n    x = BAM(x, num_filters)  # Apply BAM\n    return x","metadata":{"execution":{"iopub.status.busy":"2024-10-28T14:03:11.549726Z","iopub.execute_input":"2024-10-28T14:03:11.550726Z","iopub.status.idle":"2024-10-28T14:03:11.557619Z","shell.execute_reply.started":"2024-10-28T14:03:11.550658Z","shell.execute_reply":"2024-10-28T14:03:11.556600Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_unet(input_shape):\n    inputs = Input(input_shape)\n\n    s1, p1 = encoder_block(inputs, 64)\n    s2, p2 = encoder_block(p1, 128)\n    s3, p3 = encoder_block(p2, 256)\n    s4, p4 = encoder_block(p3, 512)\n\n    b1 = conv_block(p4, 1024)\n    b1 = conv_block(b1, 1024)\n\n    d1 = decoder_block(b1, s4, 512)\n    d2 = decoder_block(d1, s3, 256)\n    d3 = decoder_block(d2, s2, 128)\n    d4 = decoder_block(d3, s1, 64)\n\n    outputs = Conv2D(8, (2, 2), padding=\"same\", activation=\"softmax\")(d4)\n    model = Model(inputs, outputs, name='UNET_BAM')\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-10-28T14:03:13.584313Z","iopub.execute_input":"2024-10-28T14:03:13.585179Z","iopub.status.idle":"2024-10-28T14:03:13.592051Z","shell.execute_reply.started":"2024-10-28T14:03:13.585138Z","shell.execute_reply":"2024-10-28T14:03:13.591066Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_shape = (height, width, 3)\nmodel = build_unet(input_shape)","metadata":{"execution":{"iopub.status.busy":"2024-10-28T14:03:16.144519Z","iopub.execute_input":"2024-10-28T14:03:16.145379Z","iopub.status.idle":"2024-10-28T14:03:16.921384Z","shell.execute_reply.started":"2024-10-28T14:03:16.145334Z","shell.execute_reply":"2024-10-28T14:03:16.920504Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-10-28T14:03:18.104612Z","iopub.execute_input":"2024-10-28T14:03:18.105535Z","iopub.status.idle":"2024-10-28T14:03:18.372294Z","shell.execute_reply.started":"2024-10-28T14:03:18.105483Z","shell.execute_reply":"2024-10-28T14:03:18.371458Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"opt = tf.keras.optimizers.Adam(lr)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=[\"sparse_categorical_accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2024-10-28T14:03:19.884188Z","iopub.execute_input":"2024-10-28T14:03:19.885086Z","iopub.status.idle":"2024-10-28T14:03:19.898965Z","shell.execute_reply.started":"2024-10-28T14:03:19.885044Z","shell.execute_reply":"2024-10-28T14:03:19.898135Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"callbacks = [\n    ModelCheckpoint(model_file, verbose=1, save_best_only=True),\n    ReduceLROnPlateau(monitor=\"val_loss\", mode='auto', factor=0.1, patience=4),\n    CSVLogger(log_file),\n    EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n]","metadata":{"execution":{"iopub.status.busy":"2024-10-28T14:03:22.484706Z","iopub.execute_input":"2024-10-28T14:03:22.485105Z","iopub.status.idle":"2024-10-28T14:03:22.491260Z","shell.execute_reply.started":"2024-10-28T14:03:22.485069Z","shell.execute_reply":"2024-10-28T14:03:22.490045Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = model.fit(\n    train_dataset,\n    validation_data=valid_dataset,\n    epochs=epochs,\n    callbacks=callbacks,\n    verbose=1\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-28T14:03:23.595115Z","iopub.execute_input":"2024-10-28T14:03:23.595507Z","iopub.status.idle":"2024-10-28T16:49:23.746246Z","shell.execute_reply.started":"2024-10-28T14:03:23.595470Z","shell.execute_reply":"2024-10-28T16:49:23.745283Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"datasetpath = \"/kaggle/input/uavid-v1/uavid_test\"\nsave_path = os.path.join(\"Prediction\", \"modified_uavid_dataset\")\nmodel_file = \"/kaggle/working/files/modified_uavid_dataset/UnetModel.keras\"\ncreate_dir(save_path)","metadata":{"execution":{"iopub.status.busy":"2024-10-28T16:59:40.687252Z","iopub.execute_input":"2024-10-28T16:59:40.687894Z","iopub.status.idle":"2024-10-28T16:59:40.692788Z","shell.execute_reply.started":"2024-10-28T16:59:40.687854Z","shell.execute_reply":"2024-10-28T16:59:40.691731Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ncustom_objects = {\n    \"custom_BAM\": BAM,\n}\nmodel = tf.keras.models.load_model(model_file, custom_objects=custom_objects)","metadata":{"execution":{"iopub.status.busy":"2024-10-28T16:59:42.151187Z","iopub.execute_input":"2024-10-28T16:59:42.152216Z","iopub.status.idle":"2024-10-28T17:00:14.325233Z","shell.execute_reply.started":"2024-10-28T16:59:42.152171Z","shell.execute_reply":"2024-10-28T17:00:14.324382Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport os\n# Load the CSV log file\nlog_file = os.path.join(files_dir, \"Log-Unet.csv\")\nlog_data = pd.read_csv(log_file)\n\n# Check available columns in the CSV\nprint(log_data.columns)\n\n# Plot Training and Validation Loss\nplt.figure(figsize=(6, 6))\nplt.plot(log_data['loss'], label='Training Loss')\nplt.plot(log_data['val_loss'], label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\n# Save the Loss plot\nloss_plot_file_path = os.path.join(files_dir, 'training_validation_loss.png')\nplt.savefig(loss_plot_file_path) # Close the figure to free memory\nplt.show()\n\n# Plot Training and Validation Accuracy\nplt.figure(figsize=(6, 6))\nplt.plot(log_data['sparse_categorical_accuracy'], label='Training Accuracy')\nplt.plot(log_data['val_sparse_categorical_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\n# Save the Accuracy plot\naccuracy_plot_file_path = os.path.join(files_dir, 'training_validation_accuracy.png')\nplt.savefig(accuracy_plot_file_path)  # Close the figure to free memory\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-28T17:00:21.627940Z","iopub.execute_input":"2024-10-28T17:00:21.628332Z","iopub.status.idle":"2024-10-28T17:00:22.452407Z","shell.execute_reply.started":"2024-10-28T17:00:21.628293Z","shell.execute_reply":"2024-10-28T17:00:22.451537Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_x = sorted(glob(os.path.join(dataset_path, \"uavid_test\", \"*\", \"Images\", \"*\")))\nprint(f\"Test: {len(test_x)}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-28T17:00:38.567151Z","iopub.execute_input":"2024-10-28T17:00:38.568002Z","iopub.status.idle":"2024-10-28T17:00:38.705552Z","shell.execute_reply.started":"2024-10-28T17:00:38.567958Z","shell.execute_reply":"2024-10-28T17:00:38.704474Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.colors as mcolors\n\n# Define image dimensions\nwidth = 512\nheight = 512\n\n# Define RGB to class index mapping\nrgb_to_class = {\n    (0, 0, 0): 0,         # Background clutter\n    (128, 0, 0): 1,       # Building (Red)\n    (128, 64, 128): 2,    # Road (Purple)\n    (0, 128, 0): 3,       # Tree (Green)\n    (128, 128, 0): 4,     # Low vegetation (Yellow)\n    (64, 0, 128): 5,      # Moving car (Purple-blue)\n    (192, 0, 192): 6,     # Static car (Pink)\n    (64, 64, 0): 7        # Human (Dark Yellow)\n}\n\n# Reverse mapping to go from class indices to RGB\nclass_to_rgb = {v: k for k, v in rgb_to_class.items()}\n\n# Normalize RGB values for use in the colormap\nclass_colors = {k: tuple(v/255.0 for v in rgb) for k, rgb in class_to_rgb.items()}\ncolors = np.array([class_colors[i] for i in sorted(class_colors.keys())])\ncmap = mcolors.ListedColormap(colors)\nnorm = mcolors.BoundaryNorm(boundaries=np.arange(len(class_colors)+1) - 0.5, ncolors=len(class_colors))\n\n# Function to map class indices back to RGB colors\ndef map_class_to_rgb(class_mask):\n    rgb_mask = np.zeros((class_mask.shape[0], class_mask.shape[1], 3), dtype=np.uint8)\n    for class_index, rgb in class_to_rgb.items():\n        rgb_mask[class_mask == class_index] = rgb\n    return rgb_mask\n\n# Prediction code\nplt.figure(figsize=(15, 10))  # Adjust size to fit 4 images comfortably\n\n# Take a batch of images and masks from the validation dataset\nbatch = next(iter(valid_dataset))  # Fetch the next batch from the dataset\nbatch_x, batch_y = batch\n\n# Ensure we process all images in the batch\nnum_images = batch_x.shape[0]\n\nfor i in range(num_images):\n    # Extract images and masks\n    image = batch_x[i].numpy()\n    mask = batch_y[i].numpy()\n\n    # Model prediction\n    prediction = model.predict(np.expand_dims(image, axis=0))[0]  # Predict for a single image\n    predicted_class_indices = np.argmax(prediction, axis=-1)  # Convert softmax output to class indices\n\n    # Map predicted class indices to RGB\n    predicted_mask_rgb = map_class_to_rgb(predicted_class_indices)\n\n    # Load the original label image for comparison\n    original_label_path = valid_y[i]  # Make sure `valid_y` has the correct paths\n    original_label = cv2.imread(original_label_path, cv2.IMREAD_COLOR)\n    original_label = cv2.cvtColor(original_label, cv2.COLOR_BGR2RGB)\n    original_label = cv2.resize(original_label, (width, height)) / 255.0\n\n    # Plot the images\n    plt.subplot(num_images, 3, 3*i + 1)\n    plt.imshow(image)\n    plt.title(f\"Input Image {i+1}\")\n    plt.axis(\"off\")\n\n    plt.subplot(num_images, 3, 3*i + 2)\n    plt.imshow(original_label)\n    plt.title(f\"Original Label {i+1}\")\n    plt.axis(\"off\")\n\n    plt.subplot(num_images, 3, 3*i + 3)\n    plt.imshow(predicted_mask_rgb)\n    plt.title(f\"Predicted Mask {i+1}\")\n    plt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-28T17:00:39.966872Z","iopub.execute_input":"2024-10-28T17:00:39.967263Z","iopub.status.idle":"2024-10-28T17:00:49.892576Z","shell.execute_reply.started":"2024-10-28T17:00:39.967226Z","shell.execute_reply":"2024-10-28T17:00:49.891640Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"time_taken = []\nfor x in tqdm(test_x):\n    # Extract sequence folder and image name\n    seq_folder = x.split(\"/\")[-3]\n    image_name = x.split(\"/\")[-1]\n    \n    # Read and preprocess the image\n    x = cv2.imread(x, cv2.IMREAD_COLOR)\n    x = cv2.resize(x, (width, height))\n    x = x / 255.0\n    x = np.expand_dims(x, axis=0)\n\n    # Predict and measure time\n    start_time = time.time()\n    p = model.predict(x)[0]  # Remove batch dimension\n    total_time = time.time() - start_time\n    time_taken.append(total_time)\n\n    # Convert softmax output to class indices\n    p_class_indices = np.argmax(p, axis=-1)  # Convert to a single-channel mask of class indices\n    \n    # Map class indices back to RGB colors using the color map\n    p_rgb = np.zeros((p_class_indices.shape[0], p_class_indices.shape[1], 3), dtype=np.uint8)\n    \n    # Correctly map class indices to RGB colors\n    for rgb, idx in color_map.items():\n        p_rgb[p_class_indices == idx] = rgb  # Map class index to corresponding RGB value\n    \n    # Ensure it's in RGB before saving\n    p_rgb = cv2.cvtColor(p_rgb, cv2.COLOR_RGB2BGR)\n\n    # Save the mask as an RGB image, including the sequence folder in the name\n    save_path_with_name = os.path.join(save_path, f\"{seq_folder}_{image_name}\")\n    cv2.imwrite(save_path_with_name, p_rgb)","metadata":{"execution":{"iopub.status.busy":"2024-10-28T17:01:01.882810Z","iopub.execute_input":"2024-10-28T17:01:01.883621Z","iopub.status.idle":"2024-10-28T17:02:06.467593Z","shell.execute_reply.started":"2024-10-28T17:01:01.883576Z","shell.execute_reply":"2024-10-28T17:02:06.466644Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mean_time = np.mean(time_taken)\nprint(f\"Mean time taken: {mean_time}\")\nmean_fps = 1/mean_time\nprint(f\"Mean FPS: {mean_fps}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-28T17:02:21.468042Z","iopub.execute_input":"2024-10-28T17:02:21.468449Z","iopub.status.idle":"2024-10-28T17:02:21.474179Z","shell.execute_reply.started":"2024-10-28T17:02:21.468408Z","shell.execute_reply":"2024-10-28T17:02:21.473271Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r file.zip /kaggle/working\nfrom IPython.display import FileLink\nFileLink(r'file.zip')","metadata":{"execution":{"iopub.status.busy":"2024-10-28T17:02:25.006368Z","iopub.execute_input":"2024-10-28T17:02:25.006787Z","iopub.status.idle":"2024-10-28T17:02:55.616737Z","shell.execute_reply.started":"2024-10-28T17:02:25.006745Z","shell.execute_reply":"2024-10-28T17:02:55.615553Z"},"trusted":true},"outputs":[],"execution_count":null}]}