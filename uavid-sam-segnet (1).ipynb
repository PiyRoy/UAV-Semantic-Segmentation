{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2125129,"sourceType":"datasetVersion","datasetId":1077128}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom glob import glob\nimport numpy as np\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Add, GlobalAveragePooling2D, Dense, MaxPooling2D, Input, UpSampling2D\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport time\nimport matplotlib.colors as mcolors\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Model","metadata":{"execution":{"iopub.status.busy":"2024-11-12T13:53:51.195694Z","iopub.execute_input":"2024-11-12T13:53:51.196781Z","iopub.status.idle":"2024-11-12T13:53:51.202858Z","shell.execute_reply.started":"2024-11-12T13:53:51.196727Z","shell.execute_reply":"2024-11-12T13:53:51.201752Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 4\nlr = 1e-3\nepochs = 100\nwidth = 512\nheight = 512","metadata":{"execution":{"iopub.status.busy":"2024-11-12T13:59:51.676284Z","iopub.execute_input":"2024-11-12T13:59:51.677074Z","iopub.status.idle":"2024-11-12T13:59:51.681331Z","shell.execute_reply.started":"2024-11-12T13:59:51.677036Z","shell.execute_reply":"2024-11-12T13:59:51.680357Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_path = os.path.join(\"/kaggle/input/uavid-v1\")\nfiles_dir = os.path.join(\"files\", \"modified_uavid_dataset\")\nmodel_file = os.path.join(files_dir, \"UnetModel.keras\")\nlog_file = os.path.join(files_dir, \"Log-Unet.csv\")\n\n# Function to create directory\ndef create_dir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n        \ncreate_dir(files_dir)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T13:59:52.375964Z","iopub.execute_input":"2024-11-12T13:59:52.376597Z","iopub.status.idle":"2024-11-12T13:59:52.382133Z","shell.execute_reply.started":"2024-11-12T13:59:52.376559Z","shell.execute_reply":"2024-11-12T13:59:52.381178Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_data(path):\n    train_x = sorted(glob(os.path.join(path, \"uavid_train\",\"seq1\", \"Images\", \"*\")))\n    train_y = sorted(glob(os.path.join(path, \"uavid_train\",\"seq1\", \"Labels\", \"*\")))\n\n    valid_x = sorted(glob(os.path.join(path, \"uavid_val\", \"seq16\", \"Images\", \"*\")))\n    valid_y = sorted(glob(os.path.join(path, \"uavid_val\", \"seq16\", \"Labels\", \"*\")))\n\n    return (train_x, train_y), (valid_x, valid_y)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T13:59:53.035761Z","iopub.execute_input":"2024-11-12T13:59:53.036163Z","iopub.status.idle":"2024-11-12T13:59:53.042919Z","shell.execute_reply.started":"2024-11-12T13:59:53.036127Z","shell.execute_reply":"2024-11-12T13:59:53.041825Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"(train_x, train_y), (valid_x, valid_y) = load_data(dataset_path)\n\ntrain_x, additional_valid_x, train_y, additional_valid_y = train_test_split(\n    train_x, train_y, test_size=100, random_state=42\n)\n\nvalid_x = np.concatenate([valid_x, additional_valid_x], axis=0)\nvalid_y = np.concatenate([valid_y, additional_valid_y], axis=0)\n\n\nprint(f\"New Train: {len(train_x)} - {len(train_y)}\")\nprint(f\"New Valid: {len(valid_x)} - {len(valid_y)}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-12T13:59:53.916590Z","iopub.execute_input":"2024-11-12T13:59:53.917362Z","iopub.status.idle":"2024-11-12T13:59:53.935750Z","shell.execute_reply.started":"2024-11-12T13:59:53.917324Z","shell.execute_reply":"2024-11-12T13:59:53.934819Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"color_map = {\n    (0, 0, 0): 0,         # Background clutter\n    (128, 0, 0): 1,       # Building\n    (128, 64, 128): 2,    # Road\n    (0, 128, 0): 3,       # Tree\n    (128, 128, 0): 4,     # Low vegetation\n    (64, 0, 128): 5,      # Moving car\n    (192, 0, 192): 6,     # Static car\n    (64, 64, 0): 7        # Human\n}\n\ndef read_mask(path):\n    path = path.decode()\n    mask = cv2.imread(path)  \n    mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB) \n    mask = cv2.resize(mask, (width, height), interpolation=cv2.INTER_NEAREST)\n\n    class_indices = np.zeros((height, width), dtype=np.uint8)\n\n    for rgb, idx in color_map.items():\n        class_indices[(mask == rgb).all(axis=-1)] = idx\n\n    return class_indices.astype(np.uint8)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T13:59:54.776516Z","iopub.execute_input":"2024-11-12T13:59:54.777413Z","iopub.status.idle":"2024-11-12T13:59:54.784981Z","shell.execute_reply.started":"2024-11-12T13:59:54.777363Z","shell.execute_reply":"2024-11-12T13:59:54.783955Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tf_parse(x, y):\n    def _parse(x, y):\n        x = read_image(x) \n        y = read_mask(y) \n        return x, y\n    \n    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.uint8]) \n    x.set_shape([height, width, 3]) \n    y.set_shape([height, width])   \n    return x, y","metadata":{"execution":{"iopub.status.busy":"2024-11-12T13:59:55.676087Z","iopub.execute_input":"2024-11-12T13:59:55.676477Z","iopub.status.idle":"2024-11-12T13:59:55.682784Z","shell.execute_reply.started":"2024-11-12T13:59:55.676440Z","shell.execute_reply":"2024-11-12T13:59:55.681696Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def read_image(path):\n    path = path.decode()\n    x = cv2.imread(path, cv2.IMREAD_COLOR)\n    x = cv2.resize(x, (width, height))\n    x = x / 255.0\n    x = x.astype(np.float32)\n    return x","metadata":{"execution":{"iopub.status.busy":"2024-11-12T13:59:56.496063Z","iopub.execute_input":"2024-11-12T13:59:56.496450Z","iopub.status.idle":"2024-11-12T13:59:56.501738Z","shell.execute_reply.started":"2024-11-12T13:59:56.496412Z","shell.execute_reply":"2024-11-12T13:59:56.500815Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tf_dataset(x, y, batch=6):\n    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n    dataset = dataset.map(tf_parse, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.batch(batch)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2024-11-12T13:59:57.256046Z","iopub.execute_input":"2024-11-12T13:59:57.256434Z","iopub.status.idle":"2024-11-12T13:59:57.262925Z","shell.execute_reply.started":"2024-11-12T13:59:57.256398Z","shell.execute_reply":"2024-11-12T13:59:57.261824Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = tf_dataset(train_x, train_y, batch=batch_size)\nvalid_dataset = tf_dataset(valid_x, valid_y, batch=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T13:59:57.896473Z","iopub.execute_input":"2024-11-12T13:59:57.897225Z","iopub.status.idle":"2024-11-12T13:59:57.932702Z","shell.execute_reply.started":"2024-11-12T13:59:57.897186Z","shell.execute_reply":"2024-11-12T13:59:57.931809Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def encoder_block(filters, kernel_size=(3, 3), activation='relu'):\n  return tf.keras.Sequential([\n      layers.Conv2D(filters, kernel_size, padding='same'),\n      layers.BatchNormalization(),\n      layers.Activation(activation),\n      layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n  ])","metadata":{"execution":{"iopub.status.busy":"2024-11-12T13:59:58.796078Z","iopub.execute_input":"2024-11-12T13:59:58.796468Z","iopub.status.idle":"2024-11-12T13:59:58.802307Z","shell.execute_reply.started":"2024-11-12T13:59:58.796430Z","shell.execute_reply":"2024-11-12T13:59:58.801256Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def decoder_block(filters, kernel_size=(3, 3), activation='relu'):\n  return tf.keras.Sequential([\n      layers.UpSampling2D((2, 2)),\n      layers.Conv2D(filters, kernel_size, padding='same'),\n      layers.BatchNormalization(),\n      layers.Activation(activation)\n  ])","metadata":{"execution":{"iopub.status.busy":"2024-11-12T14:00:00.381602Z","iopub.execute_input":"2024-11-12T14:00:00.382538Z","iopub.status.idle":"2024-11-12T14:00:00.387718Z","shell.execute_reply.started":"2024-11-12T14:00:00.382495Z","shell.execute_reply":"2024-11-12T14:00:00.386667Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@tf.keras.utils.register_keras_serializable(package='Custom', name='SelfAttention')\n\nclass SelfAttention(layers.Layer):\n    def __init__(self, embed_dim, **kwargs):\n        super(SelfAttention, self).__init__(**kwargs)  # Accept additional arguments like 'trainable'\n        self.query_dense = layers.Conv2D(embed_dim, kernel_size=1)\n        self.key_dense = layers.Conv2D(embed_dim, kernel_size=1)\n        self.value_dense = layers.Conv2D(embed_dim, kernel_size=1)\n        self.softmax = layers.Softmax(axis=-1)\n\n    def call(self, inputs):\n        batch_size = tf.shape(inputs)[0]\n        height = tf.shape(inputs)[1]\n        width = tf.shape(inputs)[2]\n        channels = inputs.shape[-1]  # Keep channels as a static dimension to avoid shape issues\n\n        # Compute Q, K, V matrices\n        query = self.query_dense(inputs)\n        key = self.key_dense(inputs)\n        value = self.value_dense(inputs)\n\n        # Reshape for attention calculation\n        query_flattened = tf.reshape(query, (batch_size, height * width, -1))\n        key_flattened = tf.reshape(key, (batch_size, height * width, -1))\n        value_flattened = tf.reshape(value, (batch_size, height * width, -1))\n\n        # Attention Scores (scaled dot-product): Q * K^T / sqrt(d_k)\n        score = tf.matmul(query_flattened, key_flattened, transpose_b=True)\n        score = score / tf.math.sqrt(tf.cast(tf.shape(key_flattened)[-1], tf.float32))\n\n        # Apply softmax to get attention weights\n        attention_weights = self.softmax(score)\n\n        # Multiply attention weights with values\n        attention_output = tf.matmul(attention_weights, value_flattened)\n\n        # Reshape back to original input shape\n        attention_output = tf.reshape(attention_output, (batch_size, height, width, inputs.shape[-1]))\n    \n        # Combine input with attention output\n        output = inputs + attention_output\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-11-12T14:01:21.296009Z","iopub.execute_input":"2024-11-12T14:01:21.296400Z","iopub.status.idle":"2024-11-12T14:01:21.308323Z","shell.execute_reply.started":"2024-11-12T14:01:21.296364Z","shell.execute_reply":"2024-11-12T14:01:21.307325Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def SegNet(input_shape):\n    inputs = layers.Input(shape=input_shape)\n\n    # Encoder with SelfAttention only on e3 and e4\n    e1 = encoder_block(64)(inputs)\n    e2 = encoder_block(128)(e1)\n    e3 = encoder_block(256)(e2)\n    # e3 = SelfAttention(256)(e3)  # SelfAttention after encoder block 3\n    e4 = encoder_block(512)(e3)\n    e4 = SelfAttention(512)(e4)  # SelfAttention after encoder block 4\n    \n    # Decoder with SelfAttention only before final concatenation\n    d1 = decoder_block(512)(e4)\n    d1 = layers.concatenate([d1, e3], axis=-1)\n    d2 = decoder_block(256)(d1)\n    d2 = layers.concatenate([d2, e2], axis=-1)\n    d3 = decoder_block(128)(d2)\n    # d3 = SelfAttention(128)(d3)  # SelfAttention before concatenating with e1\n    d3 = layers.concatenate([d3, e1], axis=-1)\n    d4 = decoder_block(64)(d3)\n\n    outputs = layers.Conv2D(8, (3, 3), padding='same', activation='softmax')(d4)\n\n    model = Model(inputs=inputs, outputs=outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-11-12T14:02:48.536465Z","iopub.execute_input":"2024-11-12T14:02:48.536884Z","iopub.status.idle":"2024-11-12T14:02:48.545323Z","shell.execute_reply.started":"2024-11-12T14:02:48.536847Z","shell.execute_reply":"2024-11-12T14:02:48.544372Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_shape = (height, width, 3)\nmodel = SegNet(input_shape)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T14:02:49.426510Z","iopub.execute_input":"2024-11-12T14:02:49.426933Z","iopub.status.idle":"2024-11-12T14:02:49.729547Z","shell.execute_reply.started":"2024-11-12T14:02:49.426893Z","shell.execute_reply":"2024-11-12T14:02:49.728721Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-11-12T14:02:50.416851Z","iopub.execute_input":"2024-11-12T14:02:50.417243Z","iopub.status.idle":"2024-11-12T14:02:50.457078Z","shell.execute_reply.started":"2024-11-12T14:02:50.417206Z","shell.execute_reply":"2024-11-12T14:02:50.456093Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"opt = tf.keras.optimizers.Adam(lr)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=[\"sparse_categorical_accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2024-11-12T14:02:51.281109Z","iopub.execute_input":"2024-11-12T14:02:51.281743Z","iopub.status.idle":"2024-11-12T14:02:51.291266Z","shell.execute_reply.started":"2024-11-12T14:02:51.281701Z","shell.execute_reply":"2024-11-12T14:02:51.290513Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"callbacks = [\n    ModelCheckpoint(model_file, verbose=1, save_best_only=True),\n    ReduceLROnPlateau(monitor=\"val_loss\", mode='auto', factor=0.1, patience=4),\n    CSVLogger(log_file),\n    EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n]","metadata":{"execution":{"iopub.status.busy":"2024-11-12T14:02:53.075706Z","iopub.execute_input":"2024-11-12T14:02:53.076624Z","iopub.status.idle":"2024-11-12T14:02:53.081516Z","shell.execute_reply.started":"2024-11-12T14:02:53.076582Z","shell.execute_reply":"2024-11-12T14:02:53.080571Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = model.fit(\n    train_dataset,\n    validation_data=valid_dataset,\n    epochs=epochs,\n    callbacks=callbacks,\n    verbose=1,\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T14:02:53.981498Z","iopub.execute_input":"2024-11-12T14:02:53.981936Z","iopub.status.idle":"2024-11-12T15:06:42.202846Z","shell.execute_reply.started":"2024-11-12T14:02:53.981896Z","shell.execute_reply":"2024-11-12T15:06:42.201833Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"datasetpath = \"/kaggle/input/uavid-v1/uavid_test\"\nsave_path = os.path.join(\"Prediction\", \"modified_uavid_dataset\")\nmodel_file = \"/kaggle/working/files/modified_uavid_dataset/UnetModel.keras\"\ncreate_dir(save_path)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T15:19:27.123894Z","iopub.execute_input":"2024-11-12T15:19:27.124564Z","iopub.status.idle":"2024-11-12T15:19:27.129235Z","shell.execute_reply.started":"2024-11-12T15:19:27.124522Z","shell.execute_reply":"2024-11-12T15:19:27.128126Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tf.keras.config.enable_unsafe_deserialization()\ncustom_objects = {\n    \"SelfAttention\": SelfAttention\n}\nmodel = tf.keras.models.load_model(model_file, custom_objects=custom_objects)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T15:19:28.426161Z","iopub.execute_input":"2024-11-12T15:19:28.427034Z","iopub.status.idle":"2024-11-12T15:19:30.187621Z","shell.execute_reply.started":"2024-11-12T15:19:28.426991Z","shell.execute_reply":"2024-11-12T15:19:30.186617Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n# Load the CSV log file\nlog_file = os.path.join(files_dir, \"Log-Unet.csv\")\nlog_data = pd.read_csv(log_file)\n# Check available columns in the CSV\nprint(log_data.columns)\n\n# Plot Training and Validation Loss\nplt.figure(figsize=(6, 6))\nplt.plot(log_data['loss'], label='Training Loss')\nplt.plot(log_data['val_loss'], label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\n\n# Save the Loss plot\nloss_plot_file_path = os.path.join(files_dir, 'training_validation_loss.png')\nplt.savefig(loss_plot_file_path)\nplt.show()\n\n# Plot Training and Validation Accuracy\nplt.figure(figsize=(6, 6))\nplt.plot(log_data['sparse_categorical_accuracy'], label='Training Accuracy')\nplt.plot(log_data['val_sparse_categorical_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\n\n# Save the Accuracy plot\naccuracy_plot_file_path = os.path.join(files_dir, 'training_validation_accuracy.png')\nplt.savefig(accuracy_plot_file_path)  # Close the figure to free memory\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-12T15:19:33.739838Z","iopub.execute_input":"2024-11-12T15:19:33.740535Z","iopub.status.idle":"2024-11-12T15:19:34.514225Z","shell.execute_reply.started":"2024-11-12T15:19:33.740488Z","shell.execute_reply":"2024-11-12T15:19:34.513302Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_x = sorted(glob(os.path.join(dataset_path, \"uavid_test\", \"*\", \"Images\", \"*\")))\nprint(f\"Test: {len(test_x)}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-12T15:19:39.484613Z","iopub.execute_input":"2024-11-12T15:19:39.485672Z","iopub.status.idle":"2024-11-12T15:19:39.843735Z","shell.execute_reply.started":"2024-11-12T15:19:39.485619Z","shell.execute_reply":"2024-11-12T15:19:39.842651Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"time_taken = []\nfor x in tqdm(test_x):\n    \n    seq_folder = x.split(\"/\")[-3]\n    image_name = x.split(\"/\")[-1]\n    \n    x = cv2.imread(x, cv2.IMREAD_COLOR)\n    x = cv2.resize(x, (width, height))\n    x = x / 255.0\n    x = np.expand_dims(x, axis=0)\n\n    start_time = time.time()\n    p = model.predict(x)[0] \n    total_time = time.time() - start_time\n    time_taken.append(total_time)\n\n    p_class_indices = np.argmax(p, axis=-1)  \n    \n    p_rgb = np.zeros((p_class_indices.shape[0], p_class_indices.shape[1], 3), dtype=np.uint8)\n    \n    for rgb, idx in color_map.items():\n        p_rgb[p_class_indices == idx] = rgb \n    \n    p_rgb = cv2.cvtColor(p_rgb, cv2.COLOR_RGB2BGR)\n\n    save_path_with_name = os.path.join(save_path, f\"{seq_folder}_{image_name}\")\n    cv2.imwrite(save_path_with_name, p_rgb)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T15:19:40.859286Z","iopub.execute_input":"2024-11-12T15:19:40.859951Z","iopub.status.idle":"2024-11-12T15:20:57.468335Z","shell.execute_reply.started":"2024-11-12T15:19:40.859908Z","shell.execute_reply":"2024-11-12T15:20:57.467410Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_to_rgb = {v: k for k, v in color_map.items()}\n\nclass_colors = {k: tuple(v/255.0 for v in rgb) for k, rgb in class_to_rgb.items()}\ncolors = np.array([class_colors[i] for i in sorted(class_colors.keys())])\ncmap = mcolors.ListedColormap(colors)\nnorm = mcolors.BoundaryNorm(boundaries=np.arange(len(class_colors)+1) - 0.5, ncolors=len(class_colors))\n\ndef map_class_to_rgb(class_mask):\n    rgb_mask = np.zeros((class_mask.shape[0], class_mask.shape[1], 3), dtype=np.uint8)\n    for class_index, rgb in class_to_rgb.items():\n        rgb_mask[class_mask == class_index] = rgb\n    return rgb_mask\n\n\nplt.figure(figsize=(15, 10))  \n\nbatch = next(iter(valid_dataset)) \nbatch_x, batch_y = batch\n\nnum_images = batch_x.shape[0]\n\nfor i in range(num_images):\n\n    image = batch_x[i].numpy()\n    mask = batch_y[i].numpy()\n\n\n    prediction = model.predict(np.expand_dims(image, axis=0))[0]  \n    predicted_class_indices = np.argmax(prediction, axis=-1) \n\n    predicted_mask_rgb = map_class_to_rgb(predicted_class_indices)\n\n    original_label_path = valid_y[i] \n    original_label = cv2.imread(original_label_path, cv2.IMREAD_COLOR)\n    original_label = cv2.cvtColor(original_label, cv2.COLOR_BGR2RGB)\n    original_label = cv2.resize(original_label, (width, height)) / 255.0\n\n    plt.subplot(num_images, 3, 3*i + 1)\n    plt.imshow(image)\n    plt.title(f\"Input Image {i+1}\")\n    plt.axis(\"off\")\n\n    plt.subplot(num_images, 3, 3*i + 2)\n    plt.imshow(original_label)\n    plt.title(f\"Original Label {i+1}\")\n    plt.axis(\"off\")\n\n    plt.subplot(num_images, 3, 3*i + 3)\n    plt.imshow(predicted_mask_rgb)\n    plt.title(f\"Predicted Mask {i+1}\")\n    plt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-12T15:20:59.928901Z","iopub.execute_input":"2024-11-12T15:20:59.929286Z","iopub.status.idle":"2024-11-12T15:21:03.339878Z","shell.execute_reply.started":"2024-11-12T15:20:59.929250Z","shell.execute_reply":"2024-11-12T15:21:03.338866Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r SAMsegnet.zip /kaggle/working\nfrom IPython.display import FileLink\nFileLink(r'SAMsegnet.zip')","metadata":{"execution":{"iopub.status.busy":"2024-11-12T15:21:20.440207Z","iopub.execute_input":"2024-11-12T15:21:20.440958Z","iopub.status.idle":"2024-11-12T15:21:25.991326Z","shell.execute_reply.started":"2024-11-12T15:21:20.440918Z","shell.execute_reply":"2024-11-12T15:21:25.990129Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}