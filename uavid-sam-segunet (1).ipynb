{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2125129,"sourceType":"datasetVersion","datasetId":1077128}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom glob import glob\nimport numpy as np\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Add, GlobalAveragePooling2D, Dense, MaxPooling2D, Input, UpSampling2D\nfrom tensorflow.keras import layers\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport time\nimport matplotlib.colors as mcolors\nimport pandas as pd\nfrom keras.models import Model\nfrom keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPooling2D, UpSampling2D, Concatenate, Reshape","metadata":{"execution":{"iopub.status.busy":"2024-11-10T14:18:45.774283Z","iopub.execute_input":"2024-11-10T14:18:45.774928Z","iopub.status.idle":"2024-11-10T14:18:45.781869Z","shell.execute_reply.started":"2024-11-10T14:18:45.774884Z","shell.execute_reply":"2024-11-10T14:18:45.780730Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 5\nlr = 1e-3\nepochs = 100\nwidth = 512\nheight = 512","metadata":{"execution":{"iopub.status.busy":"2024-11-10T14:18:46.558797Z","iopub.execute_input":"2024-11-10T14:18:46.559172Z","iopub.status.idle":"2024-11-10T14:18:46.563738Z","shell.execute_reply.started":"2024-11-10T14:18:46.559136Z","shell.execute_reply":"2024-11-10T14:18:46.562746Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_path = os.path.join(\"/kaggle/input/uavid-v1\")\nfiles_dir = os.path.join(\"files\", \"modified_uavid_dataset\")\nmodel_file = os.path.join(files_dir, \"UnetModel.keras\")\nlog_file = os.path.join(files_dir, \"Log-Unet.csv\")\n\n# Function to create directory\ndef create_dir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n        \ncreate_dir(files_dir)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T14:18:47.499407Z","iopub.execute_input":"2024-11-10T14:18:47.499797Z","iopub.status.idle":"2024-11-10T14:18:47.506180Z","shell.execute_reply.started":"2024-11-10T14:18:47.499761Z","shell.execute_reply":"2024-11-10T14:18:47.505042Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_data(path):\n    train_x = sorted(glob(os.path.join(path, \"uavid_train\",\"seq1\", \"Images\", \"*\")))\n    train_y = sorted(glob(os.path.join(path, \"uavid_train\",\"seq1\", \"Labels\", \"*\")))\n\n    valid_x = sorted(glob(os.path.join(path, \"uavid_val\", \"seq16\", \"Images\", \"*\")))\n    valid_y = sorted(glob(os.path.join(path, \"uavid_val\", \"seq16\", \"Labels\", \"*\")))\n\n    return (train_x, train_y), (valid_x, valid_y)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T14:18:48.439373Z","iopub.execute_input":"2024-11-10T14:18:48.440115Z","iopub.status.idle":"2024-11-10T14:18:48.446252Z","shell.execute_reply.started":"2024-11-10T14:18:48.440077Z","shell.execute_reply":"2024-11-10T14:18:48.445151Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"(train_x, train_y), (valid_x, valid_y) = load_data(dataset_path)\n\ntrain_x, additional_valid_x, train_y, additional_valid_y = train_test_split(\n    train_x, train_y, test_size=100, random_state=42\n)\n\nvalid_x = np.concatenate([valid_x, additional_valid_x], axis=0)\nvalid_y = np.concatenate([valid_y, additional_valid_y], axis=0)\n\n\nprint(f\"New Train: {len(train_x)} - {len(train_y)}\")\nprint(f\"New Valid: {len(valid_x)} - {len(valid_y)}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-10T14:18:49.359198Z","iopub.execute_input":"2024-11-10T14:18:49.359615Z","iopub.status.idle":"2024-11-10T14:18:49.378366Z","shell.execute_reply.started":"2024-11-10T14:18:49.359552Z","shell.execute_reply":"2024-11-10T14:18:49.377352Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def read_image(path):\n    path = path.decode()\n    x = cv2.imread(path, cv2.IMREAD_COLOR)\n    x = cv2.resize(x, (width, height))\n    x = x / 255.0\n    x = x.astype(np.float32)\n    return x","metadata":{"execution":{"iopub.status.busy":"2024-11-10T14:18:50.463829Z","iopub.execute_input":"2024-11-10T14:18:50.464211Z","iopub.status.idle":"2024-11-10T14:18:50.469868Z","shell.execute_reply.started":"2024-11-10T14:18:50.464176Z","shell.execute_reply":"2024-11-10T14:18:50.468898Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"color_map = {\n    (0, 0, 0): 0,         # Background clutter\n    (128, 0, 0): 1,       # Building\n    (128, 64, 128): 2,    # Road\n    (0, 128, 0): 3,       # Tree\n    (128, 128, 0): 4,     # Low vegetation\n    (64, 0, 128): 5,      # Moving car\n    (192, 0, 192): 6,     # Static car\n    (64, 64, 0): 7        # Human\n}\n\ndef read_mask(path):\n    path = path.decode()\n    mask = cv2.imread(path)  \n    mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB) \n    mask = cv2.resize(mask, (width, height), interpolation=cv2.INTER_NEAREST)\n\n    class_indices = np.zeros((height, width), dtype=np.uint8)\n\n    for rgb, idx in color_map.items():\n        class_indices[(mask == rgb).all(axis=-1)] = idx\n\n    return class_indices.astype(np.uint8) ","metadata":{"execution":{"iopub.status.busy":"2024-11-10T14:18:51.919291Z","iopub.execute_input":"2024-11-10T14:18:51.919694Z","iopub.status.idle":"2024-11-10T14:18:51.927843Z","shell.execute_reply.started":"2024-11-10T14:18:51.919657Z","shell.execute_reply":"2024-11-10T14:18:51.926871Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tf_parse(x, y):\n    def _parse(x, y):\n        x = read_image(x) \n        y = read_mask(y) \n        return x, y\n    \n    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.uint8]) \n    x.set_shape([height, width, 3]) \n    y.set_shape([height, width])   \n    return x, y","metadata":{"execution":{"iopub.status.busy":"2024-11-10T14:18:53.199385Z","iopub.execute_input":"2024-11-10T14:18:53.200115Z","iopub.status.idle":"2024-11-10T14:18:53.205750Z","shell.execute_reply.started":"2024-11-10T14:18:53.200077Z","shell.execute_reply":"2024-11-10T14:18:53.204778Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tf_dataset(x, y, batch=6):\n    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n    dataset = dataset.map(tf_parse, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.batch(batch)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2024-11-10T14:18:54.218828Z","iopub.execute_input":"2024-11-10T14:18:54.219477Z","iopub.status.idle":"2024-11-10T14:18:54.224756Z","shell.execute_reply.started":"2024-11-10T14:18:54.219439Z","shell.execute_reply":"2024-11-10T14:18:54.223823Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = tf_dataset(train_x, train_y, batch=batch_size)\nvalid_dataset = tf_dataset(valid_x, valid_y, batch=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T14:18:55.259174Z","iopub.execute_input":"2024-11-10T14:18:55.260050Z","iopub.status.idle":"2024-11-10T14:18:55.296560Z","shell.execute_reply.started":"2024-11-10T14:18:55.260009Z","shell.execute_reply":"2024-11-10T14:18:55.295629Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SelfAttention(layers.Layer):\n    def __init__(self, embed_dim):\n        super(SelfAttention, self).__init__()\n        self.query_dense = layers.Conv2D(embed_dim, kernel_size=1)\n        self.key_dense = layers.Conv2D(embed_dim, kernel_size=1)\n        self.value_dense = layers.Conv2D(embed_dim, kernel_size=1)\n        self.softmax = layers.Softmax(axis=-1)\n\n    def call(self, inputs):\n        batch_size = tf.shape(inputs)[0]\n        height = tf.shape(inputs)[1]\n        width = tf.shape(inputs)[2]\n        channels = inputs.shape[-1]  # Keep channels as a static dimension to avoid shape issues\n\n        # Compute Q, K, V matrices\n        query = self.query_dense(inputs)\n        key = self.key_dense(inputs)\n        value = self.value_dense(inputs)\n\n        # Reshape for attention calculation\n        query_flattened = tf.reshape(query, (batch_size, height * width, -1))\n        key_flattened = tf.reshape(key, (batch_size, height * width, -1))\n        value_flattened = tf.reshape(value, (batch_size, height * width, -1))\n\n        # Attention Scores (scaled dot-product): Q * K^T / sqrt(d_k)\n        score = tf.matmul(query_flattened, key_flattened, transpose_b=True)\n        score = score / tf.math.sqrt(tf.cast(tf.shape(key_flattened)[-1], tf.float32))\n\n        # Apply softmax to get attention weights\n        attention_weights = self.softmax(score)\n\n        # Multiply attention weights with values\n        attention_output = tf.matmul(attention_weights, value_flattened)\n\n        # Reshape back to original input shape\n        attention_output = tf.reshape(attention_output, (batch_size, height, width, inputs.shape[-1]))\n    \n        # Combine input with attention output\n        output = inputs + attention_output\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-11-10T14:18:56.019165Z","iopub.execute_input":"2024-11-10T14:18:56.019822Z","iopub.status.idle":"2024-11-10T14:18:56.030586Z","shell.execute_reply.started":"2024-11-10T14:18:56.019779Z","shell.execute_reply":"2024-11-10T14:18:56.029626Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def SegUNet(input_shape, n_labels, kernel=3, pool_size=(2, 2), output_mode=\"softmax\"):\n    inputs = Input(shape=input_shape)\n    \n    # Encoder\n    conv1 = Conv2D(32, (kernel, kernel), padding=\"same\")(inputs)\n    conv1 = BatchNormalization()(conv1)\n    conv1 = Activation(\"relu\")(conv1)\n    conv2 = Conv2D(32, (kernel, kernel), padding=\"same\")(conv1)\n    conv2 = BatchNormalization()(conv2)\n    conv2 = Activation(\"relu\")(conv2)\n    pool1 = MaxPooling2D(pool_size)(conv2)\n    \n    conv3 = Conv2D(64, (kernel, kernel), padding=\"same\")(pool1)\n    conv3 = BatchNormalization()(conv3)\n    conv3 = Activation(\"relu\")(conv3)\n    conv4 = Conv2D(64, (kernel, kernel), padding=\"same\")(conv3)\n    conv4 = BatchNormalization()(conv4)\n    conv4 = Activation(\"relu\")(conv4)\n    pool2 = MaxPooling2D(pool_size)(conv4)\n    \n    conv5 = Conv2D(128, (kernel, kernel), padding=\"same\")(pool2)\n    conv5 = BatchNormalization()(conv5)\n    conv5 = Activation(\"relu\")(conv5)\n    conv6 = Conv2D(128, (kernel, kernel), padding=\"same\")(conv5)\n    conv6 = BatchNormalization()(conv6)\n    conv6 = Activation(\"relu\")(conv6)\n    pool3 = MaxPooling2D(pool_size)(conv6)\n    \n    conv7 = Conv2D(256, (kernel, kernel), padding=\"same\")(pool3)\n    conv7 = BatchNormalization()(conv7)\n    conv7 = Activation(\"relu\")(conv7)\n    # Apply SAM at this bottleneck layer\n    conv7 = SelfAttention(embed_dim=256)(conv7)\n    \n    pool4 = MaxPooling2D(pool_size)(conv7)\n    \n    # Bottleneck\n    conv8 = Conv2D(512, (kernel, kernel), padding=\"same\")(pool4)\n    conv8 = BatchNormalization()(conv8)\n    conv8 = Activation(\"relu\")(conv8)\n    conv9 = Conv2D(512, (kernel, kernel), padding=\"same\")(conv8)\n    conv9 = BatchNormalization()(conv9)\n    conv9 = Activation(\"relu\")(conv9)\n    # Apply SAM at the main bottleneck layer\n    conv9 = SelfAttention(embed_dim=512)(conv9)\n    \n    # Decoder\n    up1 = UpSampling2D(pool_size)(conv9)\n    concat1 = Concatenate()([up1, conv7])\n    \n    conv10 = Conv2D(256, (kernel, kernel), padding=\"same\")(concat1)\n    conv10 = BatchNormalization()(conv10)\n    conv10 = Activation(\"relu\")(conv10)\n    conv11 = Conv2D(256, (kernel, kernel), padding=\"same\")(conv10)\n    conv11 = BatchNormalization()(conv11)\n    conv11 = Activation(\"relu\")(conv11)\n    # Apply SAM in the decoder bottleneck\n    conv11 = SelfAttention(embed_dim=256)(conv11)\n    \n    up2 = UpSampling2D(pool_size)(conv11)\n    concat2 = Concatenate()([up2, conv6])\n    \n    conv12 = Conv2D(128, (kernel, kernel), padding=\"same\")(concat2)\n    conv12 = BatchNormalization()(conv12)\n    conv12 = Activation(\"relu\")(conv12)\n    conv13 = Conv2D(128, (kernel, kernel), padding=\"same\")(conv12)\n    conv13 = BatchNormalization()(conv13)\n    conv13 = Activation(\"relu\")(conv13)\n    # Optional: Apply SAM if memory allows\n    # conv13 = SelfAttention(embed_dim=128)(conv13)\n    \n    up3 = UpSampling2D(pool_size)(conv13)\n    concat3 = Concatenate()([up3, conv4])\n    \n    conv14 = Conv2D(64, (kernel, kernel), padding=\"same\")(concat3)\n    conv14 = BatchNormalization()(conv14)\n    conv14 = Activation(\"relu\")(conv14)\n    conv15 = Conv2D(64, (kernel, kernel), padding=\"same\")(conv14)\n    conv15 = BatchNormalization()(conv15)\n    conv15 = Activation(\"relu\")(conv15)\n    # Optional: Apply SAM if memory allows\n    # conv15 = SelfAttention(embed_dim=64)(conv15)\n    \n    up4 = UpSampling2D(pool_size)(conv15)\n    concat4 = Concatenate()([up4, conv2])\n    \n    conv16 = Conv2D(32, (kernel, kernel), padding=\"same\")(concat4)\n    conv16 = BatchNormalization()(conv16)\n    conv16 = Activation(\"relu\")(conv16)\n    conv17 = Conv2D(32, (kernel, kernel), padding=\"same\")(conv16)\n    conv17 = BatchNormalization()(conv17)\n    conv17 = Activation(\"relu\")(conv17)\n    \n    outputs = Conv2D(n_labels, (1, 1), activation=output_mode)(conv17)\n    \n    model = Model(inputs, outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-11-10T14:27:15.639674Z","iopub.execute_input":"2024-11-10T14:27:15.640373Z","iopub.status.idle":"2024-11-10T14:27:15.661681Z","shell.execute_reply.started":"2024-11-10T14:27:15.640331Z","shell.execute_reply":"2024-11-10T14:27:15.660671Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_shape = (512, 512, 3)\nmodel = SegUNet(input_shape, 8)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T14:27:16.608009Z","iopub.execute_input":"2024-11-10T14:27:16.608650Z","iopub.status.idle":"2024-11-10T14:27:17.203058Z","shell.execute_reply.started":"2024-11-10T14:27:16.608610Z","shell.execute_reply":"2024-11-10T14:27:17.202186Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-11-10T14:27:17.543842Z","iopub.execute_input":"2024-11-10T14:27:17.544215Z","iopub.status.idle":"2024-11-10T14:27:17.646801Z","shell.execute_reply.started":"2024-11-10T14:27:17.544180Z","shell.execute_reply":"2024-11-10T14:27:17.645788Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"opt = tf.keras.optimizers.Adam(lr)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=[\"sparse_categorical_accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2024-11-10T14:27:19.084662Z","iopub.execute_input":"2024-11-10T14:27:19.085087Z","iopub.status.idle":"2024-11-10T14:27:19.095640Z","shell.execute_reply.started":"2024-11-10T14:27:19.085051Z","shell.execute_reply":"2024-11-10T14:27:19.094727Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"callbacks = [\n    ModelCheckpoint(model_file, verbose=1, save_best_only=True),\n    ReduceLROnPlateau(monitor=\"val_loss\", mode='auto', factor=0.1, patience=4),\n    CSVLogger(log_file),\n    EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n]","metadata":{"execution":{"iopub.status.busy":"2024-11-10T14:27:20.944317Z","iopub.execute_input":"2024-11-10T14:27:20.945189Z","iopub.status.idle":"2024-11-10T14:27:20.950320Z","shell.execute_reply.started":"2024-11-10T14:27:20.945149Z","shell.execute_reply":"2024-11-10T14:27:20.949189Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = model.fit(\n    train_dataset,\n    validation_data=valid_dataset,\n    epochs=epochs,\n    callbacks=callbacks,\n    verbose=1,\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T14:27:21.964033Z","iopub.execute_input":"2024-11-10T14:27:21.964394Z","iopub.status.idle":"2024-11-10T16:16:59.321741Z","shell.execute_reply.started":"2024-11-10T14:27:21.964361Z","shell.execute_reply":"2024-11-10T16:16:59.320835Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"datasetpath = \"/kaggle/input/uavid-v1/uavid_test\"\nsave_path = os.path.join(\"Prediction\", \"modified_uavid_dataset\")\nmodel_file = \"/kaggle/working/files/modified_uavid_dataset/UnetModel.keras\"\ncreate_dir(save_path)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T16:28:00.745712Z","iopub.execute_input":"2024-11-10T16:28:00.746665Z","iopub.status.idle":"2024-11-10T16:28:00.751456Z","shell.execute_reply.started":"2024-11-10T16:28:00.746623Z","shell.execute_reply":"2024-11-10T16:28:00.750103Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@tf.keras.utils.register_keras_serializable(package='Custom', name='SelfAttention')\n\nclass SelfAttention(layers.Layer):\n    def __init__(self, embed_dim, **kwargs):\n        super(SelfAttention, self).__init__(**kwargs)  # Accept additional arguments like 'trainable'\n        self.query_dense = layers.Conv2D(embed_dim, kernel_size=1)\n        self.key_dense = layers.Conv2D(embed_dim, kernel_size=1)\n        self.value_dense = layers.Conv2D(embed_dim, kernel_size=1)\n        self.softmax = layers.Softmax(axis=-1)\n\n    def call(self, inputs):\n        batch_size = tf.shape(inputs)[0]\n        height = tf.shape(inputs)[1]\n        width = tf.shape(inputs)[2]\n        channels = inputs.shape[-1]  # Keep channels as a static dimension to avoid shape issues\n\n        # Compute Q, K, V matrices\n        query = self.query_dense(inputs)\n        key = self.key_dense(inputs)\n        value = self.value_dense(inputs)\n\n        # Reshape for attention calculation\n        query_flattened = tf.reshape(query, (batch_size, height * width, -1))\n        key_flattened = tf.reshape(key, (batch_size, height * width, -1))\n        value_flattened = tf.reshape(value, (batch_size, height * width, -1))\n\n        # Attention Scores (scaled dot-product): Q * K^T / sqrt(d_k)\n        score = tf.matmul(query_flattened, key_flattened, transpose_b=True)\n        score = score / tf.math.sqrt(tf.cast(tf.shape(key_flattened)[-1], tf.float32))\n\n        # Apply softmax to get attention weights\n        attention_weights = self.softmax(score)\n\n        # Multiply attention weights with values\n        attention_output = tf.matmul(attention_weights, value_flattened)\n\n        # Reshape back to original input shape\n        attention_output = tf.reshape(attention_output, (batch_size, height, width, inputs.shape[-1]))\n    \n        # Combine input with attention output\n        output = inputs + attention_output\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-11-10T17:08:40.514428Z","iopub.execute_input":"2024-11-10T17:08:40.514844Z","iopub.status.idle":"2024-11-10T17:08:40.526516Z","shell.execute_reply.started":"2024-11-10T17:08:40.514806Z","shell.execute_reply":"2024-11-10T17:08:40.525657Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tf.keras.config.enable_unsafe_deserialization()\ncustom_objects = {\n    \"SelfAttention\": SelfAttention\n}\nmodel = tf.keras.models.load_model(model_file, custom_objects=custom_objects)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T17:09:13.315006Z","iopub.execute_input":"2024-11-10T17:09:13.315784Z","iopub.status.idle":"2024-11-10T17:09:17.074192Z","shell.execute_reply.started":"2024-11-10T17:09:13.315743Z","shell.execute_reply":"2024-11-10T17:09:17.073184Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n# Load the CSV log file\nlog_file = os.path.join(files_dir, \"Log-Unet.csv\")\nlog_data = pd.read_csv(log_file)\n# Check available columns in the CSV\nprint(log_data.columns)\n\n# Plot Training and Validation Loss\nplt.figure(figsize=(6, 6))\nplt.plot(log_data['loss'], label='Training Loss')\nplt.plot(log_data['val_loss'], label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\n\n# Save the Loss plot\nloss_plot_file_path = os.path.join(files_dir, 'training_validation_loss.png')\nplt.savefig(loss_plot_file_path)\nplt.show()\n\n# Plot Training and Validation Accuracy\nplt.figure(figsize=(6, 6))\nplt.plot(log_data['sparse_categorical_accuracy'], label='Training Accuracy')\nplt.plot(log_data['val_sparse_categorical_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\n\n# Save the Accuracy plot\naccuracy_plot_file_path = os.path.join(files_dir, 'training_validation_accuracy.png')\nplt.savefig(accuracy_plot_file_path)  # Close the figure to free memory\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-10T17:09:27.673695Z","iopub.execute_input":"2024-11-10T17:09:27.674082Z","iopub.status.idle":"2024-11-10T17:09:28.360531Z","shell.execute_reply.started":"2024-11-10T17:09:27.674045Z","shell.execute_reply":"2024-11-10T17:09:28.359583Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_x = sorted(glob(os.path.join(dataset_path, \"uavid_test\", \"*\", \"Images\", \"*\")))\nprint(f\"Test: {len(test_x)}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-10T17:09:29.284255Z","iopub.execute_input":"2024-11-10T17:09:29.285134Z","iopub.status.idle":"2024-11-10T17:09:29.339002Z","shell.execute_reply.started":"2024-11-10T17:09:29.285093Z","shell.execute_reply":"2024-11-10T17:09:29.338044Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"time_taken = []\nfor x in tqdm(test_x):\n    \n    seq_folder = x.split(\"/\")[-3]\n    image_name = x.split(\"/\")[-1]\n    \n    x = cv2.imread(x, cv2.IMREAD_COLOR)\n    x = cv2.resize(x, (width, height))\n    x = x / 255.0\n    x = np.expand_dims(x, axis=0)\n\n    start_time = time.time()\n    p = model.predict(x)[0] \n    total_time = time.time() - start_time\n    time_taken.append(total_time)\n\n    p_class_indices = np.argmax(p, axis=-1)  \n    \n    p_rgb = np.zeros((p_class_indices.shape[0], p_class_indices.shape[1], 3), dtype=np.uint8)\n    \n    for rgb, idx in color_map.items():\n        p_rgb[p_class_indices == idx] = rgb \n    \n    p_rgb = cv2.cvtColor(p_rgb, cv2.COLOR_RGB2BGR)\n\n    save_path_with_name = os.path.join(save_path, f\"{seq_folder}_{image_name}\")\n    cv2.imwrite(save_path_with_name, p_rgb)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T17:09:30.459323Z","iopub.execute_input":"2024-11-10T17:09:30.459712Z","iopub.status.idle":"2024-11-10T17:10:32.571911Z","shell.execute_reply.started":"2024-11-10T17:09:30.459674Z","shell.execute_reply":"2024-11-10T17:10:32.570963Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_to_rgb = {v: k for k, v in color_map.items()}\n\nclass_colors = {k: tuple(v/255.0 for v in rgb) for k, rgb in class_to_rgb.items()}\ncolors = np.array([class_colors[i] for i in sorted(class_colors.keys())])\ncmap = mcolors.ListedColormap(colors)\nnorm = mcolors.BoundaryNorm(boundaries=np.arange(len(class_colors)+1) - 0.5, ncolors=len(class_colors))\n\ndef map_class_to_rgb(class_mask):\n    rgb_mask = np.zeros((class_mask.shape[0], class_mask.shape[1], 3), dtype=np.uint8)\n    for class_index, rgb in class_to_rgb.items():\n        rgb_mask[class_mask == class_index] = rgb\n    return rgb_mask\n\n\nplt.figure(figsize=(15, 10))  \n\nbatch = next(iter(valid_dataset)) \nbatch_x, batch_y = batch\n\nnum_images = batch_x.shape[0]\n\nfor i in range(num_images):\n\n    image = batch_x[i].numpy()\n    mask = batch_y[i].numpy()\n\n\n    prediction = model.predict(np.expand_dims(image, axis=0))[0]  \n    predicted_class_indices = np.argmax(prediction, axis=-1) \n\n    predicted_mask_rgb = map_class_to_rgb(predicted_class_indices)\n\n    original_label_path = valid_y[i] \n    original_label = cv2.imread(original_label_path, cv2.IMREAD_COLOR)\n    original_label = cv2.cvtColor(original_label, cv2.COLOR_BGR2RGB)\n    original_label = cv2.resize(original_label, (width, height)) / 255.0\n\n    plt.subplot(num_images, 3, 3*i + 1)\n    plt.imshow(image)\n    plt.title(f\"Input Image {i+1}\")\n    plt.axis(\"off\")\n\n    plt.subplot(num_images, 3, 3*i + 2)\n    plt.imshow(original_label)\n    plt.title(f\"Original Label {i+1}\")\n    plt.axis(\"off\")\n\n    plt.subplot(num_images, 3, 3*i + 3)\n    plt.imshow(predicted_mask_rgb)\n    plt.title(f\"Predicted Mask {i+1}\")\n    plt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-10T17:10:36.513837Z","iopub.execute_input":"2024-11-10T17:10:36.514659Z","iopub.status.idle":"2024-11-10T17:10:40.529770Z","shell.execute_reply.started":"2024-11-10T17:10:36.514610Z","shell.execute_reply":"2024-11-10T17:10:40.528797Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r SAMSEGU.zip /kaggle/working\nfrom IPython.display import FileLink\nFileLink(r'SAMSEGU.zip')","metadata":{"execution":{"iopub.status.busy":"2024-11-10T17:10:51.444006Z","iopub.execute_input":"2024-11-10T17:10:51.444763Z","iopub.status.idle":"2024-11-10T17:10:57.762446Z","shell.execute_reply.started":"2024-11-10T17:10:51.444725Z","shell.execute_reply":"2024-11-10T17:10:57.761206Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}